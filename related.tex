%!TEX root = paper.tex

\section{Conclusion and Related Work} % (fold)
\label{sec:related}
In this work, we propose a framework for improving loop invariant learning through active learning. We remark that in theory, we could learn arbitrary mathematical classifier using methods like SVM with kernel methods~\cite{svm:kernel}. Nonetheless, due to the limited proving capability of existing program verification techniques, we focus on invariants in the form of polynomial inequalities or conjunctions of polynomial inequalities.
%Furthermore, we assume there is a bound $k$ on the number of clauses in the variant.
%In practice, we would expect (refer to empirical evidence in Section~\ref{sec:evaluations}) often $k$ is of a small value.


% \begin{table}
%     \begin{center}
%     \begin{tabular}{| l | c | c | l |}
%         \hline
%         Name & Inference Strategy & Inference Source \\
%         \hline
%         ABS & Eager & \\
%         \hline
%         CONS & Eager & \\
%         \hline
%         CEGAR & Lazy & Counterexample \\
%         \hline
%         INTER & Lazy & Counterexample \\
%         \hline
%         G\&C & Eager & Empirical \\
%         \hline
%         ABD & Lazy & Semantic \\
%         \hline
%         DAL & Eager & All Above \\
%         \hline
%     \end{tabular}
%     \end{center}
%     \caption{Existing Invariant Inference Approaches}
%     \label{tab:related}
% \end{table}
This work is related to a large body of approaches on loop invariant generation. 
The existing approaches can be mainly categorized as:
Abstract Interpretation~\cite{cousot1978automatic,mine2006octagon,cousot1979systematic,karr1976affine,vincent2009subpolyhedra},
Constraint Synthesis~\cite{ashutosh2009invgen,michael2003linear,sumit2009constraint},
CounterExample Guided Abstraction Refinement (CEGAR)~\cite{henzinger2003software,thomas2001slam,edmund2003counterexample},
Program Interpolation~\cite{kenneth2010lazy,thomas2004abstractions,kenneth2003interpolation,Kenneth2006lazy},
Guess \& Check~\cite{cormac2001houdini,ernst2007daikon},
Abductive Inference~\cite{isil2013inductive}.
In this section, we compare them with our data-driven active learning approach.

On one hand, invariant inference methods based on Abstract Interpretation and Constraint Synthesis
tend to generate all possible invariants~\cite{mine2006octagon,vincent2009subpolyhedra,ashutosh2009invgen} regardless of
whether they are useful to prove the program correctness or not.
Hence, the invariants inferred by them can be too complex to generate
and thus they fail to prove the program correctness.
On the other hand, other methods based on CEGAR, Interpolation and Abduction
only generate those related to the program verification~\cite{isil2013inductive}.
Thus, they may miss some critical and necessary invariants for the program verification.
Our approach combines their strengths:
we treat the program as a black box
and label samples based on the program verification target,
i.e., the pre-conditions and the post-conditions;
we then infer all possible invariants based on the labels
to prove the program correctness.

Additionally, similar to CEGAR, Guess \& Check, Interpolation and Abduction approaches,
our active learning approach adopts an iterative refinement scheme.
After generating the invariant, we check its correctness and refine it
based on various new information (e.g., counter-examples, new samples)
if it cannot prove the program correctness.
Different from most of the existing refinement approaches,
our method is driven by data samples
rather than syntactic~\cite{cormac2001houdini} or semantic~\cite{ashutosh2009invgen,isil2013inductive} clues.
Hence, it is more flexible and extensible to capture new types of invariants,
and it is platform- and language-independent.
Based on the needs in practice, under-approximation or over-approximation
can also be applied to the data samples with ease.

% section related (end)
