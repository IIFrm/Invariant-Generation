%!TEX root = paper.tex

\section{Verification} % (fold)
\label{sec:verification}

Given a learnt predicate $Inv$, we verify whether constraint (1), (2) and (3) are satisfied using symbolic execution and constraint solving.
First we separate the given loop program with the leaned predicate into three loop-free programs,
which corresponds to constraints (1), (2) and (3). 
Then we can use symbolic execution to get path conditions and apply constraint solver to do solving.
If all of them are satisfied, we successfully get a valid invariant and also verify the program. 
Otherwise, if any of them is violated, the counterexample obtained is added to the set of sample $X$, (named as counter-example sampling in section 3)
which is then tested, categorized, used for active learning accordingly.

In implementation, we use KLEE and Z3 solver to verify the learnt predicate.
General speaking, KLEE is a symbolic virtual machine built on top of the LLVM compiler infrastructure
that can enumerate all the possible paths based on target configuration, on each of them to get all the path conditions.
We have modified KLEE source code to satisfy our demands, which can emit path conditions at given program location.
Z3 is a high-performance theorem prover being developed at Microsoft Research, which is sound and complete.
After compiling three separated programs, we apply our KLEE to get the path constraints and then submit to Z3 solver for solving.


 
The overall algorithm is presented in Figure~\ref{alg:overall}.
\begin{algorithm}[t]
\SetAlgoVlined
\Indm
\KwIn{$Pre$, $Cond$, $Body$, $Post$}
\KwOut{an invariant which completes the proof or a counterexample}
\Indp
let $S$ be a set of random samples\;
\While{true} {
    test the program for each sample in $S$\;
    \If {a state $s$ in $C$ is identified} {
        \Return $s$ as a counterexample;
    }
    let $P$, $N$ and $U$ be the respective sets accordingly\;
    %let $Inv_u = activeLearning(P, N \cup NP)$\;
    %let $Inv_o = activeLearning(P \cup NP, N)$\;
    let $Inv$ = ClassificationAlgorithm($P$, $N$)\;
    \If {$Inv$ not converged} {
        add selectiveSampling($Inv$) into $S$\;
        continue\;
    }
    %\For {each $Inv$ in $\{Inv_u, Inv_o, Inv_s\}$} {
    Divide the loop program into three based on (1)(2)(3)\;
    Run symbolic execution on each of them to get path constraints $PCs$\;
    \For {each $pc$ in $PCs$} {
        \If { $pc$ is not satisfied} {
            add the counterexample into $S$\;
            continue\;
        }
    }
        %\Else {
    \Return $Inv$ as the proof;
        %}
    
}
\caption{Algorithm $overall$}
\label{alg:overall}
\end{algorithm}


%We remark that we learn three classifiers as candidates for the loop invariant: $U$, $OU$, $O$ such that
%\begin{itemize}
%\item $U$ classifies states in $P$ and those in $N \cup NP$.
%\item $O$ classifies states in $N$ and those in $P \cup NP$.
%\item $OU$ classifies states in $P$ and $N$;
%\end{itemize}
%Intuitively, $U$ would be an under-approximation of $Inv$ (by assuming states in $NP$ does not satisfy $Inv$); 
%$O$ would be an over-approximation of $Inv$ (by assuming states in $NP$ does satisfy $Inv$); 
%and $OU$ would be an safe-approximation of $Inv$ (by using states which we are certain whether they are in $Inv$ or not).
\begin{example}
\end{example}


\begin{theorem}
Algorithm $overall$ always eventually terminates and it is correct. \hfill \qed
\end{theorem}


% section verification (end)