\section{Our Approach: Data Labelling and Candidate Verification}
\label{sec:overview}
Loop invariant generation using a guess-and-check approach is an iterative process of \emph{data collection}, \emph{classification} (i.e., \emph{guess}) and \emph{candidate verification} (i.e., \emph{check}).
%The overall workflow is shown in the Figure~\ref{fig:overview}.
In the following, we present how our approach works step-by-step and illustrate each step with the following example.

%\begin{figure}[t]
%    \centering
%    \includegraphics[scale=0.45]{figures/overview.pdf}
%    \caption{Loop Invariant Inference Framework Overview}
%    \label{fig:overview}
%\end{figure}
%%%%sunjun: the figure needs to be updated to highlight active learning

\begin{figure}[t]
\begin{subfigure}{0.5\textwidth}
    \raggedright
    \vspace{0.5cm}
%% {\scriptsize\begin{verbatim}
%% void P(int x, int y) {
%%     assume(x < y);
%%     while (x < y) {
%%         if (x < 0) x = x + 7;
%%         else x = x + 10;

%%         if (y < 0) y = y - 10;
%%         else y = y + 3;
%%     }
%%     assert(x >= y
%%         && x <= y + 16);
%% }
%% \end{verbatim}}
\vspace{-0.2cm} \[
 \begin{array}{ll}
1 & \code{void~foo(int ~x{,} ~int~y)\{} \\
2 & \code{~~~ assume(x~{<}~y);}  \\
3 & \code{~~~ while(x~{<}~y)\{}  \\
4 & \code{~~~ \quad if~(x~{<}~0)~x~{=}~x~{+}~7;}  \\
5 & \code{~~~ \quad else~ x~{=}~x~{+}~10;}\\
6 & \code{~~~ \quad if~(y~{<}~0)~y~{=}~y~{-}~10;} \\
7 & \code{~~~ \quad else~ y~{=}~y~{+}~3;}\\
8 & \code{~~~\}} \\
9 & \code{~~~assert(y~{\leq}~x~{\leq}~y~{+}~16);}\\
10  & \}
\end{array}
\]
    \vspace{-0.2cm}
    \caption{A sample program}
    \label{fig:running:example:program}
\end{subfigure}%
\begin{subfigure}{.5\textwidth}
      \centering
      \includegraphics[scale=0.42]{figures/running-sampling.pdf}
      \caption{Sampling}
      \label{fig:running:example:sampling}
\end{subfigure}
\caption{A running example}
\label{fig:running:example}
\end{figure}
%%%sunjun: the two lines should x - y = 0 and x - y = 16.

\begin{example}
An example Hoare triple is shown in Figure~\ref{fig:running:example:program} (where an \code{assume} statement captures the precondition and an \code{assert} statement captures the postcondition).
The set of variables $V$ contains two integer-type ones: $x$ and $y$. For simplicity, we write $(a, b)$ where $a$ and $b$ are integer constants to denote the  evaluation $\{x \mapsto a, y \mapsto b\}$. Further, we interpret integers in the programs as mathematical integers (i.e., they do not overflow).
%The precondition $\mathit{Pre}$ is $\mathit{x < y}$, which is the same as the loop condition $\mathit{Cond}$.
During each loop iteration, $x$ is increased by $7$ if it is negative (line 4); otherwise, it is increased by $10$ (line 5).
$y$ is decreased by $10$ if it is negative (line 6);
otherwise, it is increased by $3$ (line 7).
The postcondition $Post$ is $y \le x \le y + 16$.
This Hoare triple can be proven using a loop invariant $\mathit{Inv}$: $\mathit{x \le y + 16}$.
In the following, we show how our framework works to learn this loop invariant.
\end{example}
\noindent The overall approach is shown as Algorithm~\ref{alg:active}. We start with randomly generating a set of valuations of $V$, denoted as $SP$, at line 1 (a.k.a.~random sampling). Random sampling provides us the initial set of samples to learn the first classifier.
In this work, we have two ways to generate random samples. One is that we generate random values for each variable in $V$ based on its domain,
assuming a uniform probabilistic distribution over all values in its domain.
The other is that we use an SMT solver~\cite{barrett2009satisfiability,de2008z3} to systematically generate valuations that satisfy $Pre$
as well as those that dissatisfies $Pre$. These two ways are complementary.
On one hand, without using a solver, we may not be able to generate valuations which satisfy $Pre$ if $Pre$ is very restrictive
(or dissatisfy $Pre$ if the negation of $Pre$ is very restrictive).
On the other hand, using a solver often generates biased valuations. We remark that the cost of generating a random sample is often negligible.

Then for any valuation $s$ in $SP$, we execute the program with initial variable valuation $s$ and record the valuation of $V$ after each iteration of the loop. For any valuation $s$, we write $s \Rightarrow s'$ to denote that there exists $i \geq 0$ such that $s' = Body^i(s)$ and $Body^k(s) \models Cond$ for all $k \in [0, i)$.
That is, if we execute the program with initial valuation $s$, we obtain $s'$ after some number of iterations. We define $SP^*$ to be the set $\{s' | \exists s \in SP.~s \Rightarrow s'\}$. Next, we categorize $SP^*$ into the four disjoint sets: $CE$ (Counter-Example), $Positive$, $Negative$, $NP$ (Negative or Positive). Intuitively, $CE$ contains counterexamples which disprove the Hoare triple; $Positive$ contains those valuations of $V$ which we know must satisfy any loop invariant which proves the Hoare triple; $Negative$ contains those valuations of $V$ which we know must not satisfy any loop invariant which proves the Hoare triple; and $NP$ contains the rest. Formally,
\[
CE(SP) = \{s \in SP^* |~\exists s'.~s \Rightarrow s' \land s \models \mathit{Pre} \land s' \not \models Cond \land s' \not \models Post\} \]
$CE(SP)$ is the set of valuations such that a valuation in $CE(SP)$ satisfies $Pre$ and becomes a valuation $s'$ which fails $Post$ when the loop terminates. If $CE$ is non-empty, the Hoare triple is disproved.
%\begin{align*}
% \mathit{Positive}(\mathit{SP}) = & \{s | \exists s_0 \in SP.~\exists s'.~s_0 \Rightarrow s' \Rightarrow s \Rightarrow s_n \land \\
%     & ~~~~~~s' \models \mathit{Pre} \land s_n \not \models Cond \land s_n \models \mathit{Post}\}
%\end{align*}
%\begin{align*}
\begin{align*}
Positive(SP) = & \{s \in SP^* | \exists s_0,s_1: SP^*. \\
& ~~~~~~ s_0 \models Pre \land s_0 \Rightarrow s \Rightarrow s_1 \land s_1 \not \models Cond \land s_1 \models Post\}
\end{align*}
$Positive(SP)$ is the set of valuations such that there exists a valuation $s_0$ in $SP^*$ which satisfies $Pre$ and becomes $s$ after zero or more iterations. Furthermore, $s$ subsequently becomes $s'$ which satisfies $Post$ when the loop terminates. Let $Inv$ be any loop invariant which proves the Hoare triple. Because $s_0 \models Pre$, $s_0 \models Inv$ since $Inv$ satisfies condition (1). Since $Inv$ satisfies condition (2) and $Body(s_0) \models Inv$ if $Body(s_0) \models Cond$, by induction, we prove $s \models Inv$.
\begin{align*}
    Negative(SP) = & \{s \in SP^* | s \not \models \mathit{Pre} \land \exists s'. \\
    & ~~~~~~s \Rightarrow s' \land s' \not \models Cond \land s' \not \models Post\}
\end{align*}
$Negative(SP)$ is the set of valuations which violates $Pre$ and becomes a valuation $s'$ which violates $Post$ when the loop terminates. We show that $s \not \models Inv$ for all $Inv$ satisfying condition (1), (2) and (3). Assume that $s \in Inv$, by condition (2), $s'$ must satisfy $Inv$ through a simple induction. By condition (3), $s'$ must satisfy $Post$, which contradicts the definition of $Negative(SP)$.
\[    NP(SP) = SP^* - CE(SP) - Positive(SP) - Negative(SP)
\]
$NP(SP)$ contains the rest of the samples. We remark that a valuation $s$ in $NP$ may or may not satisfy an invariant $Inv$ which satisfies condition (1), (2) and (3).

\begin{example}
In our example, assume that the following three valuations are randomly generated:
$(1, 2)$, $(10, 1)$ and $(100, 0)$. Three sequences of valuations are generated after executing the program with these three valuations: $\langle (1, 2), (11, 5) \rangle$, $\langle (10, 1) \rangle$ and $\langle (100, 0) \rangle$.
Note that the loop is skipped entirely for the latter two cases. After categorization, set $CE(SP)$ is empty; $Positive(SP)$ is $\{(1, 2),(11, 5)\}$; $Negative(SP)$ is $\{(100, 0)\}$; and $NP(SP)$ is $\{(10, 1)\}$.
\end{example}
After obtaining the labeled samples as discussed above, method $actL(SP)$ at line 4 is invoked to generate a candidate invariant. We leave the details on this method to Section~\ref{sec:classifierlearning}. Once a candidate is identified, we move on to \emph{candidate verification}. We check whether any of the candidate invariants satisfies condition (1), (2) and (3). In particular, for each candidate invariant $c$, we check whether any of the following constraints is satisfiable or not using an $SMT$ solver~\cite{barrett2009satisfiability,de2008z3}.
\begin{align}
    & \mathit{Pre} \land \neg c \label{check:inv:pre} \\
    %% & (s \models \phi \land \mathit{Cond}) \land (\mathit{Body}(s) \not \models \phi) \label{check:inv:loop} \\
     & sp(\phi \land Cond, Body) \land \neg c \label{check:inv:loop} \\
    & c \land \neg Cond \land \neg Post \label{check:inv:post}
\end{align}
where $sp(\phi,e)$ is the strongest postcondition obtained by symbolically executing program $e$ starting from precondition $c$. If all the three constraints are unsatisfiable for any candidate invariant, we successfully prove the Hoare triple. If any of the constraints is satisfiable, a model in the form of a variable valuation is generated, which is then added to $Positive$, $Negative$, $NP$ or $CE$ accordingly. % to the execution with these valuations.
For instance, a counterexample satisfying constraint (4) is added into $Positive$ and a counterexample satisfying (6) is added into either $Negative$ or $CE$ (depending on whether it satisfies $Pre$). Afterwards, we restart from data collection, i.e., we execute the program with the counterexample valuations,
collect and add the variable valuations after each iteration of the loop to the four categories accordingly, move on to active learning and so on.
 


For instance, the candidate invariant $\mathit{x - y \geq 16}$ in our example satisfies all the three conditions and thus the Hoare triple shown in Figure~\ref{fig:running:example:program} is proved.

We remark that with the help of active learning, we can often reduce the number of learn-and-check iterations. %, and also the corresponding learning time.
For our running example, with active learning, one iteration of learn-and-check is sufficient to prove the Hoare triple.
Without active learning, multiple iterations are often required as shown in Section~\ref{sec:evaluations}.