\section{The Overall Approach}
\label{sec:overview}
Loop-invariant generation using a guess-and-check approach is an iterative process of \emph{data collection}, \emph{guessing} (i.e., classification in this work) and \emph{checking} (i.e., verification of the invariant candidate).
%The overall workflow is shown in the Figure~\ref{fig:overview}.
In the following, we present how our approach works step-by-step and illustrate each step with simple examples.

\begin{example}
A few example Hoare triples are shown in Figure~\ref{fig:running:example}, where an \code{assume} statement captures the precondition and an \code{assert} statement captures the postcondition.
The set of variables $V$ for each program contains two integer-type ones: $x$ and $y$. For simplicity, we write $(a, b)$ where $a$ and $b$ are integer constants to denote the  evaluation $\{x \mapsto a, y \mapsto b\}$. Further, we interpret integers in the programs as mathematical integers (i.e., they do not overflow).
%During each loop iteration, $x$ is increased by $7$ if it is negative (line 4); otherwise, it is increased by $10$ (line 5).
%$y$ is decreased by $10$ if it is negative (line 6);
%otherwise, it is increased by $3$ (line 7).
%The postcondition $Post$ is $y \le x \le y + 16$.
One example invariant which can be used to prove the Hoare triple is shown for each program. For instance, the Hoare triple shown in Figure~\ref{fig:running:example}(a) can be proven using a loop invariant: $x \le y + 16$, whereas conjunctive or disjunctive invariants are necessary to prove the other Hoare triples. In the following, we show how we generate loop invariants for proving these Hoare triples.
\end{example}
\noindent The overall approach is shown as Algorithm~\ref{alg:active}. We start with randomly generating a set of valuations of $V$, denoted as $SP$, at line 1 (a.k.a.~random sampling). Random sampling provides us the initial set of samples to learn the very first candidate for the loop invariant.
In this work, we have two ways to generate random samples. One is that we generate random values for each variable in $V$ based on its domain,
assuming a uniform probabilistic distribution over all values in its domain.
The other is that we apply an SMT solver~\cite{barrett2009satisfiability,de2008z3} to generate valuations that satisfy $Pre$
as well as those that fail $Pre$. These two ways are complementary.
On one hand, without using a solver, we may not be able to generate valuations which satisfy $Pre$ if $Pre$ is very restrictive
(or fail $Pre$ if the negation of $Pre$ is very restrictive). On the other hand, using a solver often generates biased valuations. %We remark that the cost of generating a random sample is often negligible.

\begin{figure}[t]
\begin{subfigure}{0.5\textwidth}
    \raggedright
\[
 \begin{array}{ll}
1 & \code{~~~ assume(x~{<}~y);}  \\
2 & \code{~~~ while(x~{<}~y)\{}  \\
3 & \code{~~~ \quad if~(x~{<}~0)~x~{:=}~x~{+}~7;}  \\
4 & \code{~~~ \quad else~ x~{:=}~x~{+}~10;}\\
5 & \code{~~~ \quad if~(y~{<}~0)~y~{:=}~y~{-}~10;} \\
6 & \code{~~~ \quad else~ y~{:=}~y~{+}~3;}\\
7 & \code{~~~\}} \\
8 & \code{~~~assert(y~{\leq}~x~{\leq}~y~{+}~16);}
\end{array}
\]
    \caption{Invariant: $x \le y + 16$}
\end{subfigure}%
\begin{subfigure}{.5\textwidth}
        \[
      \begin{array}{ll}
      1 & \code{assume(x~{>}~0~\lor~y~{>}~0);}  \\
      2 & \code{while(x~{+}~y~{\le}-2)\{}  \\
      3 & \code{\quad if~(x~{>}~0) \{}  \\
      4 & \code{\quad \quad ~~~x~{:=}~x+1;}  \\
      5 & \code{\quad \}~else ~\{} \\
      6 & \code{\quad \quad ~y {:=} y + 1;}\\
      7 & \code{\quad \}} \\
      8 & \code{\}} \\
      9 & \code{assert(x~{>}~0~ \lor ~y~{>}~0);}\\
      \end{array}
    \]
    \caption{Invariant: $x > 0 \lor y > 0$}
\end{subfigure}
   \begin{subfigure}{0.5\textwidth}
    \raggedright
     \vspace{0.3cm}
\[
 \begin{array}{ll}
1 & \code{~~~ assume(x=1 {\land} y=0);}  \\
2 & \code{~~~ while(*)\{}  \\
3 & \code{~~~ \quad x {:=} x + y;}  \\
4 & \code{~~~ \quad y {:=} y + 1;}\\
5 & \code{~~~\}} \\
6 & \code{~~~assert(x~{\geq}~y);}
\end{array}
\]
    \caption{Invariant: $y \geq 0 \land x \geq y$}
  \end{subfigure}%
   \begin{subfigure}{0.5\textwidth}
     \vspace{0.3cm}
      \[
      \begin{array}{ll}
      1 & \code{assume(x < 0);} \\
      2 & \code{while(x < 0)\{}  \\
      3 & \code{~~~ \quad x = x + y;}  \\
      4 & \code{~~~ \quad y{++};}  \\
      5 & \code{\}} \\
      6 & \code{assert(y > 0);}
      \end{array}
      \]
    \caption{Invariant: $x < 0 \lor y > 0$}
   \end{subfigure}
\caption{Example programs}
\label{fig:running:example}
\end{figure}

Next, for any valuation $s$ in $SP$, we execute the program starting with initial variable valuation $s$ and record the valuation of $V$ after each iteration of the loop. We write $s \Rightarrow s'$ to denote that there exists $i \geq 0$ such that $s' = Body^i(s)$ and $Body^k(s) \in Cond$ for all $k \in [0, i)$. That is, if we start with valuation $s$, we obtain $s'$ after some number of iterations. At line 3 of Algorithm~\ref{alg:active}, we add all such valuations $s'$ into $SP$. Next, we categorize $SP$ into the four disjoint sets: $CE$, $Positive$, $Negative$ and $NP$. Intuitively, $CE$ contains counterexamples which disprove the Hoare triple; $Positive$ contains those valuations of $V$ which we know must satisfy any loop invariant which proves the Hoare triple; $Negative$ contains those valuations of $V$ which we know must not satisfy any loop invariant which proves the Hoare triple; and $NP$ contains the rest. %Formally,
\[
CE(SP) = \{s \in SP |s \in Pre \land~\exists s'.~s \Rightarrow s' \land s' \not \in Cond \land s' \not \in Post\} \]
A valuation in $CE(SP)$ satisfies $Pre$ and becomes a valuation $s'$ which fails $Post$ when the loop terminates. If $CE(SP)$ is non-empty, the Hoare triple is disproved.
%\begin{align*}
% \mathit{Positive}(\mathit{SP}) = & \{s | \exists s_0 \in SP.~\exists s'.~s_0 \Rightarrow s' \Rightarrow s \Rightarrow s_n \land \\
%     & ~~~~~~s' \models \mathit{Pre} \land s_n \not \models Cond \land s_n \models \mathit{Post}\}
%\end{align*}
%\begin{align*}
\begin{align*}
Positive(SP) = & \{s \in SP | \exists s_0,s_1: SP. \\
& ~~~~~~ s_0 \in Pre \land s_0 \Rightarrow s \Rightarrow s_1 \land s_1 \not \in Cond \land s_1 \in Post\}
\end{align*}
$Positive(SP)$ contains a valuation $s$ if there exists a valuation $s_0$ in $SP$ which satisfies $Pre$ and becomes $s$ after zero or more iterations. Furthermore, $s$ subsequently becomes $s'$, which satisfies $Post$ when the loop terminates. Let $Inv$ be any loop invariant that proves the Hoare triple. Because $s_0 \in Pre$, $s_0 \in Inv$ since $Inv$ satisfies condition (1). Since $Inv$ satisfies condition (2) and $Body(s_0) \in Inv$ if $Body(s_0) \in Cond$. By a simple induction, we prove $s \in Inv$.
\begin{align*}
    Negative(SP) = & \{s \in SP | s \not \in Pre \land \exists s'. \\
    & ~~~~~~s \Rightarrow s' \land s' \not \in Cond \land s' \not \in Post\}
\end{align*}
$Negative(SP)$ is the set of valuations which violates $Pre$ and becomes a valuation $s'$ which violates $Post$ when the loop terminates. We show that $s \not \in Inv$ for all $Inv$ satisfying condition (1), (2) and (3). Assume that $s \in Inv$, by condition (2), $s'$ must satisfy $Inv$ through a simple induction. By condition (3), $s'$ must satisfy $Post$, which contradicts the definition of $Negative(SP)$.
\[    NP(SP) = SP - CE(SP) - Positive(SP) - Negative(SP)
\]
$NP(SP)$ contains the rest of the samples. We remark that a valuation $s$ in $NP(SP)$ may or may not satisfy an invariant $Inv$ which satisfies condition (1), (2) and (3).

\begin{algorithm}[t]
\SetAlgoVlined
\Indm
\Indp
let $SP$ be a set of randomly generated valuations of $V$\;
\While{not time out} {
    add all valuations $s'$ such that $s \Rightarrow s'$ for some $s \in SP$ into $SP$\;
    call $actL(SP)$ to generate a candidate invariant\;
    return ``proved'' if the program is verified with $\phi$ otherwise add the counterexample into $SP$\;
}
\caption{Algorithm $zilu()$}
\label{alg:active}
\end{algorithm}

\begin{example} \label{example2}
Take the program shown in Figure~\ref{fig:running:example}(a) as an example. Assume that the following three valuations are randomly generated:
$(1, 2)$, $(10, 1)$ and $(100, 0)$ at line 1. Three sequences of valuations are generated after executing the program with these three valuations: $\langle (1, 2), (11, 5) \rangle$, $\langle (10, 1) \rangle$ and $\langle (100, 0) \rangle$ respectively.
Note that the loop is skipped entirely for the latter two cases. After categorization, set $CE(SP)$ is empty; $Positive(SP)$ is $\{(1, 2),(11, 5)\}$; $Negative(SP)$ is $\{(100, 0)\}$; and $NP(SP)$ is $\{(10, 1)\}$.
\end{example}
After obtaining the samples and labeling them as discussed above, method $actL(SP)$ at line 4 in Algorithm~\ref{alg:active} is invoked to generate a candidate invariant $\phi$. We leave the details on how candidate invariants are generated in Section~\ref{sec:classifierlearning}, which is our main contribution in this work. Once a candidate is identified, we move on to check whether $\phi$ satisfies condition (1), (2) and (3) at line 5. In particular, we check whether any of the following constraints is satisfiable or not using an $SMT$ solver~\cite{barrett2009satisfiability,de2008z3}.
\begin{align}
    & \mathit{Pre} \land \neg \phi \label{check:inv:pre} \\
     & sp(\phi \land Cond, Body) \land \neg \phi \label{check:inv:loop} \\
    & \phi \land \neg Cond \land \neg Post \label{check:inv:post}
\end{align}
where $sp(\phi \land Cond,Body)$ is the strongest postcondition obtained by symbolically executing program $Body$ starting from precondition $\phi \land Cond$~\cite{DBLP:journals/cacm/Dijkstra75}. If all the three constraints are unsatisfiable, we successfully prove the Hoare triple with the loop invariant $\phi$. If any of the constraints is satisfiable, a model in the form of a variable valuation is generated, which is then added to $SP$ as a new sample. Afterwards, we restart from line 2, i.e., we execute the program with the counterexample valuations, collect and add the variable valuations after each iteration of the loop to the four categories accordingly, move on to active learning and so on.
\begin{example}
For the example shown in Figure~\ref{fig:running:example}(a), a candidate invariant which is automatically learned is $x - y \leq 16$. It is easy to check that this candidate satisfies all the three conditions and thus the Hoare triple shown in Figure~\ref{fig:running:example}(a) is proved. For Figure~\ref{fig:running:example}(c), a candidate invariant returned by method $actL(SP)$ is as follows.
\[
490 + 16x - 9y \geq 0 \land  510 + 6x + 29y \geq 0 \land 56 - y \geq 0 \land 166 - 2x + 5y \geq 0
\]
A counterexample $(-28, -11)$ is generated when we check the satisfiability of (45, which is then used to generate a new candidate. After multiple iterations of guess-and-check, the following invariant is generated.
\[
1 + 2y \geq 0 \land 1 + 2x - 2y \geq 0 \land -1 + 2x \geq 0
\]
Though different from the one we expect, this invariant turns out to be one which is strong enough to prove the Hoare triple.
\end{example}


