%!TEX root = paper.tex

\section{Classification}
After sampling and labeling, we obtain some program states must be in $Inv$ and some must not. 
Thus, any candidate invariant must be able to perfectly classify these states. 
We apply classification techniques from the machine learning community to obtain classifiers as candidate invariants.
Due to different technique can results in different forms of classifiers,
we apply SVM and some of its derivatives on our training data. 

\subsection{Linear SVM}
\subsection{Polynomial SVM}
\subsection{Conjunctive SVM}
In the following, we present how we obtain a classifier automatically using SVM. 
SVM is a supervised machine learning algorithm for classification and regression analysis. 
We use its binary classification functionality. 
Mathematically, the binary classification functionality of (linear) SVM works as follows. 
Given two sets of feature vectors $F^+$ and $F^-$, it generates, if there is any, 
a linear constraint in the form of $ax + by + \cdots \geq d$ where $x$ and $y$ are feature values and $a, b, d$ are constants, 
such that every state $s \in F^+$ satisfies the constraint and every state $s' \in F^-$ fails the constraint. 
In this work, we always choose the \textit{optimal margin classifier} (see the definition in~\cite{Sharma2012}) if possible. 
This half space could be seen as the strongest witness why the two data states are different. 
In the following, we write $svm(F^+, F^-)$ to denote the function which returns a linear classifier

If, however, $F^+$ and $F^-$ cannot be perfectly classified by one half space only, 
a more complicated function $f$ must be adopted. 
For instance, if there is a classifier in the form of conjunctive of multiple half spaces, 
the algorithm presented in~\cite{Sharma2012} can be used to identify such a classifier.

