%!TEX root = paper.tex

\section{Classification} % (fold)
\label{sec:classification}

After sampling and labeling in the last step, we obtain some program states must be in $Inv$ and some must not. 
Thinking as a data scientist, if we view these program states as a data set, 
the problem to find an invariant candidate becomes the problem to find a classifier to divide the dataset according to their labels.
Fortunately, We can borrow the idea from machine learning area as the community has studied this problem has been fully studied several years ago.
and senior machine learning experts have developed several efficient supervised classification algorithms to handle this problem.

Before coming to the specific technique $\textsc{Zilu}$ applies,
it should be noted that there are at least two main differences between traditional machine learning problems and the problem in our setting:
\begin{itemize}
\item Most machine learning techniques care more about prediction correctness rather than understanding the underlying problem,
although there is a research trend in machine learning to understand ongoing during these years.
On the contrary, in software engineering area, program verification cares about formalization and reasoning.
As a result, a classification technique would be highly suggested as long as it can predict correctly on newly received data.
But in our setting, we need an explicit classifier which is readable by human and can proceed by off-the-shelf constraints solvers.
\item Machine learning algorithms think more of generalization ability than correctness on the training data.
Sometimes they can sacrifice the prediction correctness on the training data to exchange for higher generalization ability.
But in our setting, the first thing we care about is the prediction  correctness on the training data,
meaning we can not tolerate any slight classification error on the training set.
On this basis, we hope the classification algorithm can have better generalization ability.
\end{itemize} 

Among plenty of classification approaches, $\textsc{Zilu}$ takes Supported Vector Machines as a primary approach.

\subsection{Supported Vector Machines}
%, is one of the most powerful approaches.
%SVM is a supervised learning model with associated learning algorithms that analyze data used for classification. 
%In most setting, given a set of training examples, each marked as belonging to one of two categories, 
%an SVM training algorithm builds a model, which is a representation of the examples as points in space,
%that can assign new examples into one category or the other, 
%making it a non-probabilistic binary linear classifier. 

Supported Vector Machines, known as $\textsc{Svm}$, is a supervised machine learning algorithm for classification and regression analysis. 
We use its binary classification functionality. 
Mathematically speaking, the binary classification functionality of (linear) $\textsc{Svm}$ works as follows. 
Given two sets of feature vectors $S^+$ and $S^-$, it generates, if there is any, 
a linear constraint in the form of $ax + by + \cdots \geq d$ where $x$ and $y$ are feature values and $a, b, d$ are constants, 
such that every state $s \in S^+$ satisfies the constraint and every state $s' \in S^-$ fails the constraint. 
In the following, we write $\textsc{Svm}(S^+, S^-)$ to denote the function which returns a linear classifier.

%SVM

As differences between machine learning problem and our setting are discussed in the above subsection, in order for our approach to work,
$\textsc{Zilu}$ tunes $\textsc{LibSvm}$ in the following three aspects: 
\begin{itemize}
\item $\textsc{Svm}$ technique does not really calculate a hyperplane explicitly, 
but emits some supported vectors (this is where $\textsc{Svm}$ get its name) and some parameters.
%This might not a big problem if we do not apply $\textsc{Svm}$ with some kernel method (which is used to classify no linear separable data).
However in verification, we need an explicit classifier which is readable by human and can be proceeded by off-the-shelf constraints solvers.
Thus, we need to convert the $\textsc{Svm}$ model to a hyperplane by ourselves.
(This is also the reason we do not apply $\textsc{Svm}$ with kernel methods,
as converting $\textsc{Svm}$ models with kernel methods, i.e. $\textsc{Rbf}$ kernel, would be too complicated, sometimes even impossible.)
\item As $\textsc{Zilu}$ thinks higher of the correctness on the training data than anything else,
Before applying $\textsc{Svm}$ technique, the parameters (mainly $C$ which tells the $\textsc{Svm}$ optimization how much you want to avoid misclassifying each training example)
%different from $\mathcal{C}$ used as loop invariant candidate in our context) 
should be tuned to get a classifier which does a better job of getting all the training points classified correctly.
\item What's more, we need to check the classification correctness of the learned classifier 
on $\mathcal{S}^+$ and $\mathcal{S}^-$ to ensure that is a perfect classifier.
And $\textsc{Zilu}$ also takes $\mathcal{S}^\rightarrow$ to check the classifier(as is shown in Section 2).
\end{itemize} 

So the whole classification algorithm is described in ~\ref{alg:classify}.

\begin{algorithm}[!h]
\SetAlgoVlined
\Indm
\KwIn{$\mathcal{S}^+$, $\mathcal{S}^-$, and $\mathcal{S}^\rightarrow$}
\KwOut{$\textsc{Null}$ or a perfect classifier for $\mathcal{S}^+$ and $\mathcal{S}^-$ without violating $\mathcal{S}^\rightarrow$}
\Indp
    let $f$ = $\textsc{Svm}$($\mathcal{S}^+$, $\mathcal{S}^-)$\;
    \If {$f$ violates any data in $\mathcal{S}^+$ or $\mathcal{S}^-$} {
        \Return $\textsc{Null}$\;
    }
    \If {$f$ violates any inference rules in $\mathcal{S}^\rightarrow$} {
        \Return $\textsc{Null}$\;
    }
    \Return $f$;
\caption{Algorithm $classify$}
\label{alg:classify}
\end{algorithm}

It is noted that customers can substitute $\textsc{Svm}$ with other classification techniques for learning invariant. 
We will show this in ``$\textsc{Svm}$ derivatives'' part in the end of this section.
 
%In the following, we present how we obtain a classifier automatically using $\textsc{Svm}$. 
%In this work, we always choose the \textit{optimal margin classifier} (see the definition in~\cite{Sharma2012}) if possible. 
%This half space could be seen as the strongest witness why the two data states are different. 
%In the following, we write $svm(S^+, S^-)$ to denote the function which returns a linear classifier

%\subsection{Checking}
%With the learned $\textsc{Svm}$ model,we check whether it can perfectly classify these states first.
%If yes, then we can automatically turn it back to a hyperplane form, which is regarded as our invariant candidate.
%Otherwise, we may apply other classification techniques for learning, which will be mentioned at ``$\textsc{Svm}$ derivatives'' part in the end of this section.


\subsection{Active Learning} 
After getting a classifier for current dataset, active learning technique is used to refine the guess until convergence. 
Algorithm~\ref{alg:active} presents details on how active learning is implemented in \textsc{Zilu}. 
At line 3, we obtain a classifier based on Algorithm~\ref{classify}. 
We compare the newly obtained classifier with the previous one at line 4, if they are identical, we return the classifier; 
otherwise, we apply selective sampling so that we can generate additional labeled samples for improving the classifier. 
In particular, at line 9, we apply standard techniques~\cite{DBLP:conf/icml/SchohnC00} to select the most informative sample. 
Notice that in our setting, as indicated in Section Sampling, the most informative samples are those which are exactly on the lines 
and therefore can be obtained by solving an equation system using libraries. 
At line 10 and line 11, we test the program with the newly generated samples so as to label them accordingly.
\begin{algorithm}[t]
\SetAlgoVlined
\Indm
\KwIn{$\mathcal{S}^+$, $\mathcal{S}^-$, and $\mathcal{S}^\rightarrow$}
\KwOut{an invariant candidate $\mathcal{C}$}
\Indp
let $old_f$ be $null$\;
\While{true} {
    let $f$ = classify($\mathcal{S}^+$, $\mathcal{S}^-$, $\mathcal{S}^\rightarrow$)\;
    \If {$f$ is not equal to $\textsc{Null}$} {
        \If {$f$ is identical to $old_f$} {
            $\mathcal{C}$ = $f$\;
            \Return $\mathcal{C}$;
        }
        let $old_f = f$\;
    }
  %\textsc{Re-sampling}:\\
    $sam$ = selectiveSampling($old_f$)\;
    test the target program with $sam$\;
    update $\mathcal{S}^+$, $\mathcal{S}^-$, and $\mathcal{S}^\rightarrow$ accordingly\;
}
\caption{Algorithm $activeLearning$}
\label{alg:active}
\end{algorithm}


%\begin{example}
%\LL{to be added}
%\end{example}

\begin{proposition}
Algorithm $activeLearning$ always eventually terminates. \hfill \qed
\end{proposition}


\subsection{SVM Derivatives}
%$\mathcal{S}^+$, $\mathcal{S}^-$, and $\mathcal{S}^\rightarrow$
If $\mathcal{S}^+$, $\mathcal{S}^-$ cannot be perfectly classified by one half-space only, 
a more complicated function $f$ must be adopted. 
For instance, if there is a classifier in the form of conjunctive of multiple half spaces, 
the algorithm presented in~\cite{Sharma2012} can be used to identify such a classifier.

Also in algorithm~\ref{alg:classify}, we mentioned the customers can substitute $\textsc{Svm}$ with other classification approaches for invariant learning.
$\textsc{Zilu}$ has implemented two algorithms besides native $\textsc{Svm}$: 
$Polynomial \textsc{Svm}$, which can be used to learn invariants in the form of polynomials or any equivalent expressions,
and $Conjunctive \textsc{Svm}$, which can learn invariants in the form of conjunctives.

\subsubsection{Polynomial SVM}
%In previous research, several papers ~\cite{**} have studied invariants with conjunctive form or disjunctive form.
%However, there is still no efficient approach to learning these invariants.
%In our research, we found sometimes convert the conjunctive or disjunctives to a polynomial expression might be a nice try to this problem.
In the real work, there are invariants of all kinds of form, one of which may be polynomial invariants, which have not been discussed by previous research.
In this part, we would like to learn this kind of invariants.
Apparently methods that use linear template or linear classification algorithm do not work.
%Actually, polynomials are more powerful on this than they look at the first glance, especially univariate polynomials. 
%Some cubic univariate polynomials can represent disjunctive of a conjunctive expression and a linear expression.
%For example, the following two expressions are equivalent:
%$$\big(x \ge x_0 \bigwedge x \le x_1) \bigvee x \ge x_2\big) \ where\ x_0 < x_1 < x_2$$
%$$x^3 + (x_0x_1 + x_0x_2 + x_1x_2)x^2 - (x_0 + x_1 + x_2)x - x_0x_1x_2 >= 0$$ 
This leads us to develop a classification algorithm for learning polynomial divider.
The primary idea is simple, after mapping raw data (program states) from original space in $\mathcal{S}^+$, $\mathcal{S}^-$ and $\mathcal{S}^\rightarrow$ to a high dimensions, 
we can apply primitive $\textsc{Svm}$ algorithm to learn a polynomial classifier. 
In practice, $\textsc{Zilu}$ provide polynomials up to degree 4 as we think degree 4 can cover most hackneyed invariants for now.

\begin{algorithm}[!h]
\SetAlgoVlined
\Indm
\KwIn{$\mathcal{S}^+$, $\mathcal{S}^-$}
\KwOut{a perfect polynomial classifier for $\mathcal{S}^+$ and $\mathcal{S}^-$}
\Indp
    $max_dimension$ is pre-defined\;
    $dimension = 1$\;
    \While {$dimension \le max_dimention$} {
        $\mathcal{Q}^+$ = mapToDimension($\mathcal{S}^+$, dimension)\;
        $\mathcal{Q}^-$ = mapToDimension($\mathcal{S}^+$, dimension)\;
        let $f$ = $\textsc{Svm}$($\mathcal{Q}^+$, $\mathcal{Q}^-$)\;
        \If {$f$ does not violate any raw data in $\mathcal{S}^+$ or $\mathcal{S}^-$} {
        	\Return $f$\;
    	}
    	$dimension = dimension + 1$\;
    }
    \Return $\textsc{Null}$;
\caption{Algorithm $polynomialSVM$}
\label{alg:polynomialSVM}
\end{algorithm}

Along with polynomial invariants, we found there are also some invariant of conjunctive or disjunctive form which can be expressed in polynomials,
which means our approach can also find these sort of invariants.
For instance, if the target invariant is 
$$(x \ge x_0) \vee (x \le x_1) \ where\ x_0 < x_1$$
we can find an equivalent polynomial expression in polynomial form:
$$x^2 - (x_0 + x_1)x + x_0x_1 \ge 0$$ 
So $polynomialSVM$ can find a polynomial classifier for any complex expression as long as it can be expressed in a form of polynomials.


\subsubsection{Conjunctive SVM}
Polynomials seems powerful in express abilities, 
but there actually exists simple expressions that can not expressed in form of polynomials,
For example, $\mathcal{C} = (x \ge 0 \bigwedge y \ge 0)$,
it can be proved that there is no such a polynomial which can be equivalent with it.
So in this case, Rahul Sharma and others have developed a classification algorithm called ``$\textsc{Svm\_i}$'' in ~\ref{sharma2012interpolants},
which can learn the conjunctive invariants.
In their algorithm, they take $\textsc{Svm}$ as meta classification algorithm, and they failed to simplify the learned classifiers.
So in $\textsc{Zilu}$, we apply a technique ~\ref{alg:conjunctiveSVM} based on their $\textsc{Svm\_i}$ algorithm but overcome these defects:

\begin{algorithm}[!h]
\SetAlgoVlined
\Indm
\KwIn{$\mathcal{S}^+$, $\mathcal{S}^-$}
\KwOut{a set of perfect classifiers for $\mathcal{S}^+$ and $\mathcal{S}^-$}
\Indp
    let $\mathcal{C}$ = $\textsc{Null}$\;
    let $\textsc{Misclassified}$ = $\mathcal{S}^-$\;
    \While {$\textsc{Misclassified}$ is not empty} {
        Random choose $s$ from $\textsc{Misclassified}$\;
        let $f$ = $\textsc{Svm}$($\mathcal{S}^+$, s)\;
        add $f$ to $\mathcal{C}$\;
        \For {$s' \in \textsc{Misclassified}$} {\
            \If {$f(s') \le 0$} {
                remove $s'$ from $\textsc{Misclassified}$\;
            }
        }
    }
    \For {$c \in \mathcal{C}$} {
        \If {$\mathcal{C}\diagdown c \Rightarrow c$} {
            remove $c$ from $\mathcal{C}$\;
        }
    }
    \Return $\mathcal{C}$;
\caption{Algorithm $conjunctiveSVM$}
\label{alg:conjunctiveSVM}
\end{algorithm}



%\section{Active Learning}
%Due to the limited set of samples we have (which is often referred to as labeled samples in the machine learning community), 
%the guessed classifier obtained from the previous iteration might be far from being correct. 
%In fact, without labeled samples which are right on the boundary of the `actual' classifier, 
%it is very unlikely that we would find it. 
%Intuitively, in order to get the `actual' classifier, we would require samples which would distinguish the actual one from any nearby one. 
%This problem has been discussed and addressed in the machine learning community using active learning and selective sampling~\cite{DBLP:conf/icml/SchohnC00}.

%The concept of active learning or selective sampling refers to the approaches 
%that aim at reducing the labeling effort by selecting only the most informative samples to be labeled. 
%SVM selective sampling techniques have been proven effective in achieving a high accuracy 
%with fewer examples in many applications~\cite{DBLP:conf/mm/TongC01,DBLP:journals/jmlr/TongK01}. 
%The basic idea of selective sampling is that at each round, 
%we select the samples that are the closest to the classification boundary so that they are the most difficult to classify and the most informative to be labeled. 
%Since an SVM classification function is represented by support vectors which are the samples closest to the boundary, 
%this selective sampling effectively learns an accurate function with fewer labeled data~\cite{DBLP:conf/icml/SchohnC00}. 
%In our setting, this means that we should sample a program state right by the classifier and test the program 
%with that state to label that feature vector so that the classifier would be improved.


% section classification (end)