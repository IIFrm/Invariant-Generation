%!TEX root = paper.tex

\section{Active Learning} % (fold)
\label{sec:learning}
Active learning aims at generating candidate invariants through iterations of classification and selective sampling. Our overall algorithm for active learning is shown in Algorithm~\ref{alg:active}. The inputs of the algorithm include the set of samples $\mathit{SP}$; a classification algorithm $\mathit{classify}(P,N)$ which takes two sets of samples $P$ and $N$ and generates a classifier separating $P$ and $N$; and an algorithm for selective sampling $\mathit{selectiveSampling}$ which is often coupled with the classification algorithm. During each iteration of the loop from line 2 to 10, we apply $\mathit{classify}$ three times at line 3, 4 and 5 to learn three classifiers. The reason has been discussed in Section~\ref{sec:overview}. At line 6, we check whether any of the classifiers is different from the ones obtained during the last iteration. If none of them is, we return the three classifiers as candidate invariants at line 7, which will be verified afterwards. Otherwise, at line 9, we apply selective sampling to add more samples into $\mathit{SP}$ and then move on to the next iteration. We remark this algorithm is customizable in term of the classification algorithm and the corresponding selective sampling algorithm. In the following, we present two classification algorithms and selective sampling methods as examples.

\begin{algorithm}[t]
\SetAlgoVlined
\Indm
\Indp
Initialize $\mathit{old\_f_1}, \mathit{old\_f_2}, \mathit{old\_f_3}$ as $\textsc{null}$\;
\While{true} {
    $f_1$\ := \ $\mathit{classify}(\mathit{Positive}(\mathit{SP}), \mathit{Negative}(\mathit{SP}))$\;
    $f_2$\ := \ $\mathit{classify}(\mathit{Positive}(\mathit{SP}), \mathit{Negative}(\mathit{SP}) \cup \mathit{NP}(\mathit{SP}))$\;
    $f_3$\ := \ $\mathit{classify}(\mathit{Positive}(\mathit{SP}) \cup \mathit{NP}(\mathit{SP}), \mathit{Negative}(\mathit{SP}))$\;
    \If {for all $f_i$ where $i \in \{1,2,3\}$ such that $f_i = \mathit{old\_f_i}$} {
        \Return $f_1$, $f_2$, $f_3$\;
    }
    \For {$f_i$ where $i \in \{1,2,3\}$ such that $f_i \neq \textsc{null}$ and $f_i \neq \mathit{old\_f_i}$} {
        add $\mathit{selectiveSamples}(f_i, \mathit{SP})$ into $\mathit{SP}$\;
        $\mathit{old\_f_i}\ := \ f_i$\;
    }
}
\caption{Algorithm $\mathit{activeLearning}(\mathit{SP})$}
\label{alg:active}
\end{algorithm}

\subsection{Classification}
In the machine learning community, many classification algorithms have been proposed, e.g., perceptron~\cite{perceptron}, decision tree~\cite{quinlan1986induction}, SVM~\cite{svm:original} and neutral network~\cite{nn}.
%Among plenty of classification approaches, \textsc{Zilu} takes Supported Vector Machines as a primary approach.
%Before demonstrating the specific technique applied in \textsc{Zilu},
%it should be noted that there are at least two main differences between machine learning problems and the problem in our setting:
In our framework, the classification algorithms must generate perfect classifiers. Formally, a perfect classifier $\phi$ for two sets of samples $P$ and $N$ is a predicate such that $s \models \phi$ for all $s \in P$ and $s \not \models \phi$ for all $s \in N$. Furthermore, the classification algorithms must generate classifiers which are human-interpretable or can be handled by existing program verification techniques. In the following, we present two such classification algorithms based on Support Vector Machine (SVM). We remark that our framework is not restricted to SVM-based classification algorithms.
%But there are still differences between the classification problem in machine learning field and the one in our framework:
%\begin{itemize}
%\item Our framwork need to learn an classifier with explicit form which is readable by human and can be proceeded by off-the-shelf constraints solvers.
%However, the current machine learning techniques always produce an implicit classifier that is used to do prediction.
%So we need to convert the implicit classifier to an explicit one, which will be used in later stage.
%%Most machine learning techniques values more on prediction correctness rather than understanding the underlying problem. \LL{What is the difference?}
%%although there is a research trend in machine learning to understand ongoing during these years.
%%However, in the software engineering area, program verification cares about formalization and reasoning.
%%Therefore, a classification technique from machine learning area would be highly suggested as long as it can predict correctly on newly received data.
%%But in our setting, we need an explicit classifier which is readable by human and can be proceeded by off-the-shelf constraints solvers.
%\item
%%Machine learning algorithms consider more of generalization ability than classification correctness on the training data.
%%Thus they would \LL{sacrifice the prediction correctness on the training data} for exchanging the higher generalization ability.  \LL{I feel our method is incorrect after reading this. Be precise. }
%In our context, the classification accuracy on the training data is of vital importance,
%and even a slight classification error can not be tolerated.
%But some machine learning algorithms would partially sacrifice the classification correctness on the training data
%for exchanging a higher prediction accuracy on the unseen data,
%as they values the generalization ability most.
%%On this basis, we hope the learned classifier can reflect the underlying logic of the given program.
%% classification algorithm can have better generalization ability.
%%\end{itemize}
%Considering these differences,
%in the following text,
%we show our classification approaches which extend the native SVM to learning the invariant candidate.
%\LL{I understand your meaning, but maybe say, we thus extend svm to capture ...
%Do not describe methods in a vague manner. }


%\subsection{Support Vector Machines}
%\label{subsec:svm}
%, is one of the most powerful approaches.
%SVM is a supervised learning model with associated learning algorithms that analyze data used for classification.
%In most setting, given a set of training examples, each marked as belonging to one of two categories,
%an SVM training algorithm builds a model, which is a representation of the examples as points in space,
%that can assign new examples into one category or the other,
%making it a non-probabilistic binary linear classifier.
SVM is a supervised machine learning algorithm for classification and regression analysis~\cite{svm:original}. In this work, we use its binary classification functionality.
In general, the binary classification functionality of (linear) $\textsc{Svm}$ works as follows.
Given two sets of samples $P$ and $N$, SVM generates a perfect linear classifier to separate them if there is any.
In the following, we write $\mathit{svm}(P, N)$ to denote the function which returns a perfect linear classifier for $P$ and $N$ if there is any; or returns $\textsc{null}$ otherwise.
We refer the readers to~\cite{svm:smo} for details on how the classifier is computed.
In this work, we always choose the \textit{optimal margin classifier} (see the definition in~\cite{sharma2012interpolants}) if possible.
Intuitively, the optimal margin classifier could be seen as the strongest witness why $P$ and $N$ are different.
SVM by default learns only classifiers in the form of a linear inequality (a.k.a.~a half space),
e.g., in the form of $a x + b y \geq c$ where $x$ and $y$ are variables whereas $a$, $b$, and $c$ are constants.
In practice, such classifiers may not be sufficient and thus, we develop algorithms to learn more expressive invariants.

\paragraph{Polynomial Classifiers} Algorithm~\ref{alg:conjunctiveSVM} shows our algorithm for learning classifiers in the form of polynomial inequalities.
The inputs of the algorithm include the two sets of samples $P$ and $N$ as well as a threshold on the maximum degree of the polynomial classifier.
During the loop from line 2 to 7, we first map all the samples in $P$ (similarly $N$) to a set of samples $P'$ (similarly $N'$) in a high dimensional space using function $\mathit{mapToDegree}$.
Given a set of samples $X$ and a target degree, function $\mathit{mapToDegree}$ returns a set of new samples $X'$ such that each element in $X$ is mapped to an element in $X'$.
For instance, let $\{x \mapsto 2, y \mapsto 1\}$ be an element in $X$ and the target $\mathit{degree}$ be 2, the following element is added to $X'$: $\{x \mapsto 2, y \mapsto 1, x^2 \mapsto 4, xy \mapsto 2, y^2 \mapsto 1\}$.
That is, given a valuation in $P$, we systematically add all terms constituted by the same variables with a degree no more than $\mathit{degree}$, as if they are new variables.
%The primary idea is simple, after mapping raw data (program states) from original space in $\mathcal{S}^+$, $\mathcal{S}^-$ and $\mathcal{S}^\rightarrow$ to a high dimensions,
SVM is then applied to learn a perfect linear classifier for $P'$ and $N'$. % as is shown in~\ref{subsec:svm}.
Mathematically, a linear classifier in the high dimensional space is the same as a polynomial classifier in the original space~\cite{svm:kernel}.
In order to favor classifiers with lower degrees (which often are easier to understand or verify), we start with looking for a polynomial classifier with degree 1 and only look for higher-degree polynomial classifiers if we find no lower-degree ones.
The algorithm terminates when we reach the threshold on the maximum degree.
We remark that the size of each sample in $P'$ or $N'$ grows rapidly with $\mathit{degree}$. 
%The number of monomials of $\mathit{degree}$ in $\mathit{n}$ is $(^{n+degree-1}_{degree})$.
However, in our experiments, we can often identify useful classifiers with a small value for $\mathit{degree}$.
We remark that the polynomial classifiers learned by Algorithm~\ref{alg:polynomialSVM} can represent classifiers in the form of disjunctive or conjunctive linear inequalities.
%Because some invariants of conjunctive or disjunctive form can be expressed in polynomials equivalently,
For instance, the classifier $\big((x \ge d_0) \wedge (x \le d_1)\big) \vee (x \ge d_2)$
where $d_0 < d_1 < d_2$ are constants can be represented as: $x^3 + (d_0d_1 + d_0d_2 + d_1d_2)x^2 - (d_0 + d_1 + d_2)x - d_0d_1d_2 \geq 0$.

\begin{algorithm}[t]
\SetAlgoVlined
\Indm
\Indp
    Initialize $degree$ as 1\;
    \While {$degree \le \mathit{max\_degree}$} {
        $P' := \mathit{mapToDegree}(P, \mathit{degree})$\;
        $N' := \mathit{mapToDegree}(N, \mathit{degree})$\;
        \If {$\mathit{svm}(P', N')$ is not $\textsc{null}$} {
            \Return $\mathit{svm}(P', N')$\;
        }
    	$\mathit{degree} := \mathit{degree} + 1$\;
    }
    \Return $\textsc{null}$;
\caption{Algorithm $\mathit{polynomial}(P,N)$}
\label{alg:polynomialSVM}
\end{algorithm}
\vspace{-2mm}
\begin{algorithm}[t]
\SetAlgoVlined
\Indm
\Indp
    Initialize set $C$\ as an empty set $\{\}$, set $\mathit{Misclassified}$\ as\ $N$\;
    \While {$\mathit{Misclassified}$ is not empty} {
        Random choose $s$ from $\mathit{Misclassified}$\;
        $f := \mathit{polynomial}(P, \{s\})$\;
        Add $f$ into $C$\;
        \For {$s' \in \mathit{Misclassified}$} {\
            \If {$f(s') < 0$} {
                remove $s'$ from $\mathit{Misclassified}$\;
            }
        }
    }
    Minimize $C$\;
    \Return the conjunction of all predicates in $C$;
\caption{Algorithm $\mathit{conjunctivePoly}(P, N)$}
\label{alg:conjunctiveSVM}
\end{algorithm}

%The whole classification algorithm is described in Algorithm~\ref{alg:classify}.
%Note that,
%$\textsc{Svm}$ in this algorithm can be substitute with other classification techniques for different learning purposes.
%We will take two classification algorithms as examples to demonstrate this promotion in \LL{?}~\ref{subsec:svm:derivatives}.
%\begin{algorithm}[!h]
%\SetAlgoVlined
%\Indm
%\KwIn{$\mathcal{S}^+$, $\mathcal{S}^-$, and $\mathcal{S}^\rightarrow$}
%\KwOut{$\textsc{Null}$ or a perfect classifier for $\mathcal{S}^+$ and $\mathcal{S}^-$ without violating $\mathcal{S}^\rightarrow$}
%\Indp
%    let $f$ = $\textsc{Svm}$($\mathcal{S}^+$, $\mathcal{S}^-)$\;
%    \If {$f$ violates any data in $\mathcal{S}^+$ or $\mathcal{S}^-$} {
%        \Return $\textsc{Null}$\;
%    }
%    \If {$f$ violates any inference rules in $\mathcal{S}^\rightarrow$} {
%        \Return $\textsc{Null}$\;
%    }
%    \Return $f$;
%\caption{Algorithm $classify$}
%\label{alg:classify}
%\end{algorithm}



%In the following, we present how we obtain a classifier automatically using $\textsc{Svm}$.
%In this work, we always choose the \textit{optimal margin classifier} (see the definition in~\cite{Sharma2012}) if possible.
%This half space could be seen as the strongest witness why the two data states are different.
%In the following, we write $svm(S^+, S^-)$ to denote the function which returns a linear classifier

%\subsection{Checking}
%With the learned $\textsc{Svm}$ model,we check whether it can perfectly classify these states first.
%If yes, then we can automatically turn it back to a hyperplane form, which is regarded as our invariant candidate.
%Otherwise, we may apply other classification techniques for learning, which will be mentioned at ``$\textsc{Svm}$ derivatives'' part in the end of this section.


%\subsection{SVM Derivatives}
%\label{subsec:svm:derivatives}
%%$\mathcal{S}^+$, $\mathcal{S}^-$, and $\mathcal{S}^\rightarrow$
%If $\mathcal{S}^+$, $\mathcal{S}^-$ cannot be classified with 100\% accuary by one half-space only,
%a more complicated function $f$ must be adopted.
%For instance, there has been research on invariants in form of conjunctives~\cite{sharma2012interpolants},
%octagonal~\cite{mine2006octagon}, tree form~\cite{krishna2015learning}\cite{garg2015learning} and so on.
%
%%For instance, if there is a classifier in the form of conjunctive of multiple half spaces,
%%the algorithm presented in~\cite{sharma2012interpolants} can be used to identify such a classifier.
%
%Moreover, as is noted Algorithm~\ref{alg:classify} can be easily extended by substituting $\textsc{Svm}$ with other classification approaches,
%\textsc{Zilu} has implemented two classification algorithms by extending the native $\textsc{Svm}$:
%$Polynomial \textsc{Svm}$ for learning invariants in the form of polynomials or any equivalent expressions,
%and $Conjunctive$\textsc{Svm} for learning invariants in the form of conjunctives.
%
%\medskip\noindent
%\textbf{Polynomial SVM.}
%In previous research, several papers ~\cite{**} have studied invariants with conjunctive form or disjunctive form.
%However, there is still no efficient approach to learning these invariants.
%In our research, we found sometimes convert the conjunctive or disjunctives to a polynomial expression might be a nice try to this problem.
%In the real work, there are not only linear invariants but invariants of many other forms,
%such as, conjunctives~\cite{sharma2012interpolants}, octagonal~\cite{mine2006octagon}, polynomial, and tree form~\cite{krishna2015learning}\cite{garg2015learning}.
%In this part, we present our classification approach based on primitive \textsc{Svm} for learning polynomial invariants,
%which have not been discussed by previous research.
%In this part, we would like to learn this kind of invariants.
%Apparently methods that use linear template or linear classification algorithm do not work.
%Actually, polynomials are more powerful on this than they look at the first glance, especially univariate polynomials.
%Some cubic univariate polynomials can represent disjunctive of a conjunctive expression and a linear expression.
%For example, the following two expressions are equivalent:
%$$\big(x \ge x_0 \bigwedge x \le x_1) \bigvee x \ge x_2\big) \ where\ x_0 < x_1 < x_2$$
%$$x^3 + (x_0x_1 + x_0x_2 + x_1x_2)x^2 - (x_0 + x_1 + x_2)x - x_0x_1x_2 >= 0$$
%This leads us to develop a classification algorithm for learning polynomial divider.
%
%The whole procedure is shown in algorithm~\ref{alg:polynomialSVM}.
%In practice, \textsc{Zilu} provide polynomials up to degree 4 as we think degree 4 can cover most hackneyed invariants for now.


%\begin{align}
%    x^3 + a\dot x^2 - b\dot x - c &>= &0 \\
%    where a &= & x_0 \dot x_1 + x_0 \dot x_2 + x_1 \dot x_2 \\
%\end{align}
%For instance, if the target invariant is
%$$(x \ge x_0) \vee (x \le x_1) \ where\ x_0 < x_1$$
%we can find an equivalent polynomial expression:
%$$x^2 - (x_0 + x_1)x + x_0x_1 \ge 0$$
%So $polynomialSVM$ can be used to learn a polynomial classifier or any complex expression as long as it can be expressed in form of polynomials.

\paragraph{Conjunctive Polynomial Classifiers}
As discussed above, polynomial classifiers can equivalently represent certain conjunction or disjunction of linear classifiers. It is however not always possible, i.e., some conjunctive or disjunctive linear inequalities cannot be expressed in the form of a polynomial classifier. A simple example is $x \ge 0 \land y \ge 0$. Thus, we develop an algorithm to learn more expressive classifiers.

Algorithm~\ref{alg:conjunctiveSVM} shows a classification algorithm which is designed to learn conjunctive polynomial classifiers.
%So in this part, we introduce the algorithm to learn conjunctive invariants directly.
%, avoiding the tries to convert them into a form of polynomials.
It is inspired by the algorithm presented in~\cite{sharma2012interpolants}. The difference is that~\cite{sharma2012interpolants} learns conjunctive linear classifiers, whereas we learn conjunctive polynomial classifiers. The idea is to pick one sample $s$ from $N$ (at line 3) each time and identify a polynomial classifier based on Algorithm~\ref{alg:polynomialSVM} separating $P$ and $\{s\}$ (a.k.a.~a clause). Next, we remove (at line 6 to 8) all samples from $N$ which can be correctly classified by the polynomial classifier found at line 4, and then repeat the process until $N$ becomes empty.
%Compared to the classification algorithm presented in~\cite{sharma2012interpolants}, Algorithm~\ref{alg:conjunctiveSVM} learns conjunctive polynomial classifiers whereas the one in~\cite{sharma2012interpolants} is limited to conjunctive linear classifier. The more important difference is however our classification algorithm is coupled with a selective sampling procedure.
At line 10, we minimize $C$ by identifying and removing any $f$ in $C$ such that the conjunction of the remaining predicates in $C$ perfectly classifies $P$ and $N$ still. We remark that Algorithm~\ref{alg:conjunctiveSVM}, like the one in~\cite{sharma2012interpolants}, may learn a classifier with many clauses, as one clause is introduced each time line 5 is executed. In the worse case, if each polynomial classifier found at line 4 only classifies the one sample $s$, the returned $C$ would conjunct as many clauses as the number of samples in $N$. Selective sampling, as we discuss below, helps to tame this problem, if there exists a conjunctive polynomial classifier with fewer clauses.





%\section{Active Learning}
%Due to the limited set of samples we have (which is often referred to as labeled samples in the machine learning community),
%the guessed classifier obtained from the previous iteration might be far from being correct.
%In fact, without labeled samples which are right on the boundary of the `actual' classifier,
%it is very unlikely that we would find it.
%Intuitively, in order to get the `actual' classifier, we would require samples which would distinguish the actual one from any nearby one.
%This problem has been discussed and addressed in the machine learning community using active learning and selective sampling~\cite{DBLP:conf/icml/SchohnC00}.

%The concept of active learning or selective sampling refers to the approaches
%that aim at reducing the labeling effort by selecting only the most informative samples to be labeled.
%SVM selective sampling techniques have been proven effective in achieving a high accuracy
%with fewer examples in many applications~\cite{DBLP:conf/mm/TongC01,DBLP:journals/jmlr/TongK01}.
%The basic idea of selective sampling is that at each round,
%we select the samples that are the closest to the classification boundary so that they are the most difficult to classify and the most informative to be labeled.
%Since an SVM classification function is represented by support vectors which are the samples closest to the boundary,
%this selective sampling effectively learns an accurate function with fewer labeled data~\cite{DBLP:conf/icml/SchohnC00}.
%In our setting, this means that we should sample a program state right by the classifier and test the program
%with that state to label that feature vector so that the classifier would be improved.


% section classification (end)

\subsection{Selective Sampling} \label{subsec:active:learning}
Selective sampling is helpful in reducing the number of required samples. Often, different selective sampling methods are adopted according to different classification algorithms. In the following, we show how selective sampling works for the two above-mentioned classification algorithms. %We refer the readers to~\cite{???} on a survey on how selective sampling works in general.

Assume that we adopt Algorithm~\ref{alg:polynomialSVM} in our framework and learn a polynomial classifier: $-4x^2+2y \geq -11$. Following the idea in~\cite{DBLP:conf/icml/OrabonaC11}, the following procedure is applied to identify samples right on the classification boundary for improving the classifier.
\begin{enumerate}
\item Choose a variable in the classifier, for example, $x$.
\item Generates random value for other variables. For example, we let $y$ be $12$.
\item Solve the equation $-4x^2+2y = -11$ after substituting variables with their values. If there is no solution, go back to (1) and retry. In our example, we get $x \approx 2.9580$.
%\item Add a random variance $\epsilon \in [-1, 1]$ to the value of the picked variable. Here we add $\epsilon = 0.4$ to the value of $x$, and thus the new value of $x$ is $3.3580$.
\item Roundoff the values of all the variables according to their types in the given program. In our example, we obtain the valuation $\{x \mapsto 3, y \mapsto 12\}$.
\end{enumerate}
Alternatively, we can use existing equation system solvers directly to find solutions for equation $-4x^2+2y = -11$. If Algorithm~\ref{alg:conjunctiveSVM} is adopted to learn conjunctive polynomial classifiers, we apply the above procedure to each and every polynomial clause in the classifier to obtain new samples. While it is easy to see that the classifiers learned in Algorithm~\ref{alg:active} may improve through the iterations (since more samples are available), it is hard to predict how fast it converges. We evaluate the effectiveness of these classification algorithms as well as selective sampling methods empirically in the next section.
%Having a classifier for the current dataset ($\mathcal{S}^+$, $\mathcal{S}^-$, and $\mathcal{S}^\rightarrow$),
%active learning technique keeps refining the classifier until it gets converged.
%That means the classifier remains identical even adding more data points into the training set.
%Algorithm~\ref{alg:active} presents details on how active learning is implemented in \textsc{Zilu}.
%
%
%
%At line 3, we obtain a classifier based on Algorithm~\ref{alg:classify}.
%We compare the newly obtained classifier with the previous one at line 4, if they are identical, we return the classifier;
%otherwise, we apply selective sampling so that we can generate additional labeled samples for refining the classifier.
%%\LL{should be used to check the correctness?}
%In particular, at line 9, we apply selective sampling~\cite{DBLP:conf/icml/SchohnC00} to generate most informative samples.
%Notice that in our setting, as indicated in Section ~\ref{sec:sampling}, the most informative samples are those which are exactly on the lines
%and therefore can be obtained by solving an equation system using libraries.
%At line 10 and line 11, we test the program with the newly generated samples so as to label them accordingly.

%\begin{example}
%\LL{to be added}
%\end{example}

%\begin{proposition}
%Algorithm $activeLearning$ always eventually terminates. \hfill \qed
%\end{proposition}

