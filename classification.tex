%!TEX root = paper.tex

\section{Classification} % (fold)
\label{sec:classification}

After sampling and labeling in the last step, we obtain some program states must satisfy $Inv$ and some must not. 
%If we view these program states as a data set from the perspective of data scientists, 
%the challenge to find an invariant candidate can be viewed as the problem to find a classifier to divide the dataset according to their labels.
In this section, we state our classification algorithm to find a classifier which can perfectly divide these collected data.

%Fortunately, the machine learning community has studied this problem for years, where we can borrow the idea from.
%Specially, senior machine learning experts have developed several efficient supervised classification algorithms to handle this issue.
In fact, the classification problem has been studied by machine learning community for years,
and several efficient approaches, i.e. perceptron, decision tree, \textsc{Svm} etc., have been developed to handle this problem.
%Among plenty of classification approaches, \textsc{Zilu} takes Supported Vector Machines as a primary approach.
%Before demonstrating the specific technique applied in \textsc{Zilu},
%it should be noted that there are at least two main differences between machine learning problems and the problem in our setting:
But there are still differences between the problem in machine learning field and the one in our setting:
\begin{itemize}
\item Most machine learning techniques values more on prediction correctness rather than understanding the underlying problem.
%although there is a research trend in machine learning to understand ongoing during these years.
However, in the software engineering area, program verification cares about formalization and reasoning.
Therefore, a classification technique from machine learning area would be highly suggested as long as it can predict correctly on newly received data.
But in our setting, we need an explicit classifier which is readable by human and can be proceeded by off-the-shelf constraints solvers.

\item Machine learning algorithms consider more of generalization ability than classification correctness on the training data.
Thus they would sacrifice the prediction correctness on the training data for exchanging the higher generalization ability.
But in our context, the classification accuracy on the training data is of vital importance,
and even a slight classification error can not be tolerated.
On this basis, we hope the learned classifier can reflect the underlying logic of the given program.
% classification algorithm can have better generalization ability.
\end{itemize} 
Considering these differences,
in the following text, 
we show our classification approaches based on the classical machine learning algorithms.


\subsection{SVM}
\label{subsec:svm}
%, is one of the most powerful approaches.
%SVM is a supervised learning model with associated learning algorithms that analyze data used for classification. 
%In most setting, given a set of training examples, each marked as belonging to one of two categories, 
%an SVM training algorithm builds a model, which is a representation of the examples as points in space,
%that can assign new examples into one category or the other, 
%making it a non-probabilistic binary linear classifier. 
\textsc{Svm} (Supported Vector Machines) is a supervised machine learning algorithm for classification and regression analysis. 
We use its binary classification functionality in our framework. 
Mathematically speaking, the binary classification functionality of (linear) $\textsc{Svm}$ works as follows. 

Given two sets of feature vectors $S^+$ and $S^-$, it generates, if there is any, 
a linear constraint in the form of $ax + by + \cdots \geq d$ where $x$ and $y$ are feature values and $a, b, d$ are constants, 
such that every state $s \in \mathcal{S}^+$ satisfies the constraint and every state $s' \in \mathcal{S}^-$ fails the constraint. 
In the following, we write $\textsc{Svm}(\mathcal{S}^+, \mathcal{S}^-)$ to denote the function which returns a linear classifier.


Considering the differences between machine learning problem and our setting, 
\textsc{Zilu} tunes the primitive $\textsc{LibSvm}$~\cite{chang2011libsvm} in the following three aspects in order to get a perfect classifier.
\begin{itemize}
\item Convert \textsc{Svm} model to an explicit classifier.
The original \textsc{Svm} technique does not explicitly calculate a hyperplane, but emits some supported vectors and other parameters,
which can be used to do prediction on the given data.
%This might not a big problem if we do not apply $\textsc{Svm}$ with some kernel method (which is used to classify no linear separable data).
However, we need a explicit classifier as the loop invariant candidate which can be understood and proceeded later for verification.
(This is also why we do not apply $\textsc{Svm}$ with kernel methods~\cite{yu2009evolving},
considering converting $\textsc{Svm}$ models with kernel methods, i.e. $\textsc{Rbf}$ kernel, would be a complicated, sometimes even impossible, task.)

\item As \textsc{Zilu} treasures classification accuracy on the training dataset% than anything else,
before applying primitive $\textsc{Svm}$ technique, the parameters 
(mainly $C$ which tells the $\textsc{Svm}$ optimization how much you want to avoid misclassifying each training example)
%different from $\mathcal{C}$ used as loop invariant candidate in our context) 
should be carefully tuned to learn a perfect classifier which perform well on the training points.

\item Validating the learned classifier. 
Checking the classification correctness of the learned classifier 
on $\mathcal{S}^+$ and $\mathcal{S}^-$ is still needed as our setting needs to ensure the learned classifier is a perfect one.
\textsc{Zilu} also takes $\mathcal{S}^\rightarrow$ to validate the learned classifier(as is shown in Section~\ref{sec:sampling}).
\end{itemize} 

The whole classification algorithm is described in Algorithm~\ref{alg:classify}. 
Note that,
$\textsc{Svm}$ in this algorithm can be substitute with other classification techniques for different learning purposes. 
We will take two classification algorithms as examples to demonstrate this promotion in ~\ref{subsec:svm:derivatives}.
\begin{algorithm}[!h]
\SetAlgoVlined
\Indm
\KwIn{$\mathcal{S}^+$, $\mathcal{S}^-$, and $\mathcal{S}^\rightarrow$}
\KwOut{$\textsc{Null}$ or a perfect classifier for $\mathcal{S}^+$ and $\mathcal{S}^-$ without violating $\mathcal{S}^\rightarrow$}
\Indp
    let $f$ = $\textsc{Svm}$($\mathcal{S}^+$, $\mathcal{S}^-)$\;
    \If {$f$ violates any data in $\mathcal{S}^+$ or $\mathcal{S}^-$} {
        \Return $\textsc{Null}$\;
    }
    \If {$f$ violates any inference rules in $\mathcal{S}^\rightarrow$} {
        \Return $\textsc{Null}$\;
    }
    \Return $f$;
\caption{Algorithm $classify$}
\label{alg:classify}
\end{algorithm}


 
%In the following, we present how we obtain a classifier automatically using $\textsc{Svm}$. 
%In this work, we always choose the \textit{optimal margin classifier} (see the definition in~\cite{Sharma2012}) if possible. 
%This half space could be seen as the strongest witness why the two data states are different. 
%In the following, we write $svm(S^+, S^-)$ to denote the function which returns a linear classifier

%\subsection{Checking}
%With the learned $\textsc{Svm}$ model,we check whether it can perfectly classify these states first.
%If yes, then we can automatically turn it back to a hyperplane form, which is regarded as our invariant candidate.
%Otherwise, we may apply other classification techniques for learning, which will be mentioned at ``$\textsc{Svm}$ derivatives'' part in the end of this section.


\subsection{Active Learning} 
\label{subsec:active:learning}
Having a perfect classifier for the current dataset ($\mathcal{S}^+$, $\mathcal{S}^-$, and $\mathcal{S}^\rightarrow$), 
active learning technique keeps refining the classifier until it gets converged.
That means the classifier remains identical even adding more data points into the training set. 
Algorithm~\ref{alg:active} presents details on how active learning is implemented in \textsc{Zilu}. 

\begin{algorithm}[!h]
\SetAlgoVlined
\Indm
\KwIn{$\mathcal{S}^+$, $\mathcal{S}^-$, and $\mathcal{S}^\rightarrow$}
\KwOut{an invariant candidate $\mathcal{C}$}
\Indp
let $old_f$ be $null$\;
\While{true} {
    let $f$ = classify($\mathcal{S}^+$, $\mathcal{S}^-$, $\mathcal{S}^\rightarrow$)\;
    \If {$f$ is not equal to $\textsc{Null}$} {
        \If {$f$ is identical to $old_f$} {
            $\mathcal{C}$ = $f$\;
            \Return $\mathcal{C}$;
        }
        let $old_f = f$\;
    }
  %\textsc{Re-sampling}:\\
    $sam$ = selectiveSampling($old_f$)\;
    test the target program with $sam$\;
    update $\mathcal{S}^+$, $\mathcal{S}^-$ and $\mathcal{S}^\rightarrow$ accordingly\;
}
\caption{Algorithm $activeLearning$}
\label{alg:active}
\end{algorithm}

At line 3, we obtain a classifier based on Algorithm~\ref{alg:classify}. 
We compare the newly obtained classifier with the previous one at line 4, if they are identical, we return the classifier; 
otherwise, we apply selective sampling so that we can generate additional labeled samples for improving the classifier. 
In particular, at line 9, we apply standard techniques~\cite{DBLP:conf/icml/SchohnC00} to select the most informative sample. 
Notice that in our setting, as indicated in Section ~\ref{sec:sampling}, the most informative samples are those which are exactly on the lines 
and therefore can be obtained by solving an equation system using libraries. 
At line 10 and line 11, we test the program with the newly generated samples so as to label them accordingly.

%\begin{example}
%\LL{to be added}
%\end{example}

\begin{proposition}
Algorithm $activeLearning$ always eventually terminates. \hfill \qed
\end{proposition}


\subsection{SVM Derivatives}
\label{subsec:svm:derivatives}
%$\mathcal{S}^+$, $\mathcal{S}^-$, and $\mathcal{S}^\rightarrow$
If $\mathcal{S}^+$, $\mathcal{S}^-$ cannot be perfectly classified by one half-space only, 
a more complicated function $f$ must be adopted. 
For instance, there has been research on invariants in form of conjunctives~\cite{sharma2012interpolants}, 
octagonal~\cite{mine2006octagon}, tree form~\cite{krishna2015learning}\cite{garg2015learning} and so on.

%For instance, if there is a classifier in the form of conjunctive of multiple half spaces, 
%the algorithm presented in~\cite{sharma2012interpolants} can be used to identify such a classifier.

Moreover, as is noted in algorithm~\ref{alg:classify}, customers can replace $\textsc{Svm}$ with other classification approaches for invariant learning,
\textsc{Zilu} has implemented two classification algorithms besides native $\textsc{Svm}$: 
$Polynomial \textsc{Svm}$ for learning invariants in the form of polynomials or any equivalent expressions,
and $Conjunctive \textsc{Svm}$ for learning invariants in the form of conjunctives.

\subsubsection{Polynomial SVM}
%In previous research, several papers ~\cite{**} have studied invariants with conjunctive form or disjunctive form.
%However, there is still no efficient approach to learning these invariants.
%In our research, we found sometimes convert the conjunctive or disjunctives to a polynomial expression might be a nice try to this problem.
%In the real work, there are not only linear invariants but invariants of many other forms, 
%such as, conjunctives~\cite{sharma2012interpolants}, octagonal~\cite{mine2006octagon}, polynomial, and tree form~\cite{krishna2015learning}\cite{garg2015learning}.
In this part, we present our classification approach based on primitive \textsc{Svm} for learning polynomial invariants,
which have not been discussed by previous research.
%In this part, we would like to learn this kind of invariants.
%Apparently methods that use linear template or linear classification algorithm do not work.
%Actually, polynomials are more powerful on this than they look at the first glance, especially univariate polynomials. 
%Some cubic univariate polynomials can represent disjunctive of a conjunctive expression and a linear expression.
%For example, the following two expressions are equivalent:
%$$\big(x \ge x_0 \bigwedge x \le x_1) \bigvee x \ge x_2\big) \ where\ x_0 < x_1 < x_2$$
%$$x^3 + (x_0x_1 + x_0x_2 + x_1x_2)x^2 - (x_0 + x_1 + x_2)x - x_0x_1x_2 >= 0$$ 
%This leads us to develop a classification algorithm for learning polynomial divider.
To develop a polynomial learner, 
we first map all the raw data (program states) from original space in $\mathcal{S}^+$, $\mathcal{S}^-$ and $\mathcal{S}^\rightarrow$ 
to a high dimensional space.
%The primary idea is simple, after mapping raw data (program states) from original space in $\mathcal{S}^+$, $\mathcal{S}^-$ and $\mathcal{S}^\rightarrow$ to a high dimensions, 
The primitive \textsc{Svm} algorithm is then applied on the mapped data to learn a linear classifier on the projected space.% as is shown in~\ref{subsec:svm}.
Mathematically, a linear classifier in the high dimensional space is the same with a polynomial classifier in the original space.
In this way, we get a polynomial classifier as a result.
The whole procedure is shown in algorithm~\ref{alg:polynomialSVM}.
%In practice, \textsc{Zilu} provide polynomials up to degree 4 as we think degree 4 can cover most hackneyed invariants for now.

\begin{algorithm}[!h]
\SetAlgoVlined
\Indm
\KwIn{$\mathcal{S}^+$, $\mathcal{S}^-$}
\KwOut{a perfect polynomial classifier for $\mathcal{S}^+$ and $\mathcal{S}^-$}
\Indp
    $max\_dimension$ is pre-defined\;
    $dimension = 1$\;
    \While {$dimension \le max\_dimention$} {
        $\mathcal{Q}^+$ = mapToDimension($\mathcal{S}^+$, dimension)\;
        $\mathcal{Q}^-$ = mapToDimension($\mathcal{S}^-$, dimension)\;
        let $f$ = \textsc{Svm}($\mathcal{Q}^+$, $\mathcal{Q}^-$)\;
        \If {$f$ does not violate any raw data in $\mathcal{S}^+$ or $\mathcal{S}^-$} {
        	\Return $f$\;
    	}
    	$dimension = dimension + 1$\;
    }
    \Return $\textsc{Null}$;
\caption{Algorithm $polynomial$\textsc{Svm}}
\label{alg:polynomialSVM}
\end{algorithm}

Along with learning polynomial invariants,
$polynomialSVM$ can also used to learn some invariants with conjunctive or disjunctive forms.
Because some invariants of conjunctive or disjunctive form can be expressed in polynomials equivalently,
For instance, if the target invariant is 
$$\big((x \ge x_0) \wedge (x \le x_1)\big) \vee (x \ge x_2) ~~~where\ x_0 < x_1 < x_2$$
which is really a challenge for current verification approaches to get,
we can find an equivalent polynomial expression:
$$x^3 + (x_0x_1 + x_0x_2 + x_1x_2)x^2 - (x_0 + x_1 + x_2)x - x_0x_1x_2 >= 0$$

%\begin{align}
%    x^3 + a\dot x^2 - b\dot x - c &>= &0 \\
%    where a &= & x_0 \dot x_1 + x_0 \dot x_2 + x_1 \dot x_2 \\
%\end{align}
%For instance, if the target invariant is 
%$$(x \ge x_0) \vee (x \le x_1) \ where\ x_0 < x_1$$
%we can find an equivalent polynomial expression:
%$$x^2 - (x_0 + x_1)x + x_0x_1 \ge 0$$ 
So $polynomialSVM$ can be used to learn a polynomial classifier or any complex expression as long as it can be expressed in form of polynomials.


\subsubsection{Conjunctive SVM}
As is stated in the last paragraph, polynomials can be equivalent with complex conjunctive or disjunctive of linear expressions.
%seems powerful in expression abilities, 
but there actually exists simple expressions that can not be expressed in form of polynomials.
For example, $\mathcal{C} = (x \ge 0 \wedge y \ge 0)$,
it can be proved that there is no such a polynomial which can be equivalent with it.

So in this part, we introduce the algorithm to learn conjunctive invariants directly.
%, avoiding the tries to convert them into a form of polynomials.
Our algorithm is derived from `$\textsc{Svm\_i}$' in~\cite{sharma2012interpolants}.
%Rahul Sharma, A. V. Nori et al. have developed a classification algorithm, named as `$\textsc{Svm\_i}$' in ~\cite{sharma2012interpolants},
%based on $\textsc{Svm}$ to learn conjunctive invariants.
In their algorithm, they take $\textsc{Svm}$ as meta linear classification engine which limits each part of conjunctives to be the linear form, 
and they fail to simplify the learned classifiers which can cause large number of conjunctive parts that can not be understood.
So in \textsc{Zilu}, we apply a technique overcoming these defects by applying $polynomal$\text{Svm} as meta classification algorithm 
and using Z3~\cite{de2008z3} to resolve the inference relationship between classifiers.
The technique is shown in algorithm~\ref{alg:conjunctiveSVM}.% based on their $\textsc{Svm\_i}$ but overcome these defects:

\begin{algorithm}[!h]
\SetAlgoVlined
\Indm
\KwIn{$\mathcal{S}^+$, $\mathcal{S}^-$}
\KwOut{a set of perfect classifiers for $\mathcal{S}^+$ and $\mathcal{S}^-$}
\Indp
    let $\mathcal{C}$ = $\textsc{Null}$\;
    let $\textsc{Misclassified}$ = $\mathcal{S}^-$\;
    \While {$\textsc{Misclassified}$ is not empty} {
        Random choose $s$ from $\textsc{Misclassified}$\;
        let $f$ = polynomial\textsc{Svm}($\mathcal{S}^+$, s)\;
        add $f$ to $\mathcal{C}$\;
        \For {$s' \in \textsc{Misclassified}$} {\
            \If {$f(s') \le 0$} {
                remove $s'$ from $\textsc{Misclassified}$\;
            }
        }
    %}
    \For {$c \in \mathcal{C}$} {
        \If {$\mathcal{C}\diagdown c \Rightarrow c$} {
            remove $c$ from $\mathcal{C}$\;
        }
    }
    }
    \Return $\mathcal{C}$;
\caption{Algorithm $conjunctive$\textsc{Svm}}
\label{alg:conjunctiveSVM}
\end{algorithm}



%\section{Active Learning}
%Due to the limited set of samples we have (which is often referred to as labeled samples in the machine learning community), 
%the guessed classifier obtained from the previous iteration might be far from being correct. 
%In fact, without labeled samples which are right on the boundary of the `actual' classifier, 
%it is very unlikely that we would find it. 
%Intuitively, in order to get the `actual' classifier, we would require samples which would distinguish the actual one from any nearby one. 
%This problem has been discussed and addressed in the machine learning community using active learning and selective sampling~\cite{DBLP:conf/icml/SchohnC00}.

%The concept of active learning or selective sampling refers to the approaches 
%that aim at reducing the labeling effort by selecting only the most informative samples to be labeled. 
%SVM selective sampling techniques have been proven effective in achieving a high accuracy 
%with fewer examples in many applications~\cite{DBLP:conf/mm/TongC01,DBLP:journals/jmlr/TongK01}. 
%The basic idea of selective sampling is that at each round, 
%we select the samples that are the closest to the classification boundary so that they are the most difficult to classify and the most informative to be labeled. 
%Since an SVM classification function is represented by support vectors which are the samples closest to the boundary, 
%this selective sampling effectively learns an accurate function with fewer labeled data~\cite{DBLP:conf/icml/SchohnC00}. 
%In our setting, this means that we should sample a program state right by the classifier and test the program 
%with that state to label that feature vector so that the classifier would be improved.


% section classification (end)