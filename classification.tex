%!TEX root = paper.tex

\section{Classification} % (fold)
\label{sec:classification}

After sampling and labeling in the last step, we obtain some program states must satisfy $Inv$ and some must not. 
If we view these program states as a data set from the perspective of data scientists, 
the challenge to find an invariant candidate can be viewed as the problem to find a classifier to divide the dataset according to their labels.
Fortunately, the machine learning community has studied this problem for years, where we can borrow the idea from.
Specially, senior machine learning experts have developed several efficient supervised classification algorithms to handle this issue.
Among plenty of classification approaches, \textsc{Zilu} takes Supported Vector Machines as a primary approach.

Before coming to the specific technique \textsc{Zilu} applies,
it should be noted that there are at least two main differences between machine learning problems and the problem in our setting:
\begin{itemize}
\item Most machine learning techniques care more about prediction correctness rather than understanding the underlying problem.
%although there is a research trend in machine learning to understand ongoing during these years.
On the contrary, in the software engineering area, program verification cares about formalization and reasoning.
Therefore, a classification technique would be highly suggested as long as it can predict correctly on newly received data.
But in our setting, we need an explicit classifier which is readable by human and can be proceeded by off-the-shelf constraints solvers.

\item Machine learning algorithms think more of generalization ability than classification correctness on the training data.
Sometimes they would sacrifice the prediction correctness on the training data for exchanging the higher generalization ability.
But in our context, the first thing we care about is the prediction  correctness on the training data,
which means we can not tolerate any slight classification error on the training set.
On this basis, we hope the classification algorithm can have better generalization ability.
\end{itemize} 


\subsection{Supported Vector Machines}
%, is one of the most powerful approaches.
%SVM is a supervised learning model with associated learning algorithms that analyze data used for classification. 
%In most setting, given a set of training examples, each marked as belonging to one of two categories, 
%an SVM training algorithm builds a model, which is a representation of the examples as points in space,
%that can assign new examples into one category or the other, 
%making it a non-probabilistic binary linear classifier. 
Supported Vector Machines, known as $\textsc{Svm}$, is a supervised machine learning algorithm for classification and regression analysis. 
We use its binary classification functionality. 
Mathematically speaking, the binary classification functionality of (linear) $\textsc{Svm}$ works as follows. 

Given two sets of feature vectors $S^+$ and $S^-$, it generates, if there is any, 
a linear constraint in the form of $ax + by + \cdots \geq d$ where $x$ and $y$ are feature values and $a, b, d$ are constants, 
such that every state $s \in \mathcal{S}^+$ satisfies the constraint and every state $s' \in \mathcal{S}^-$ fails the constraint. 
In the following, we write $\textsc{Svm}(\mathcal{S}^+, \mathcal{S}^-)$ to denote the function which returns a linear classifier.


Considering the differences between machine learning problem and our setting are discussed in the above subsection, 
\textsc{Zilu} tunes $\textsc{LibSvm}$~\cite{chang2011libsvm} in the following three aspects in order to get a classifier: 
\begin{itemize}
\item $\textsc{Svm}$ technique does not really calculate a hyperplane explicitly, 
but emits some supported vectors (this is where $\textsc{Svm}$ get its name) and other parameters.
%This might not a big problem if we do not apply $\textsc{Svm}$ with some kernel method (which is used to classify no linear separable data).
However, in software verification, we need to convert the $\textsc{Svm}$ model to an explicit classifier which can be understood and proceeded later.
(This is also the reason why we do not apply $\textsc{Svm}$ with kernel methods~\cite{yu2009evolving},
given converting $\textsc{Svm}$ models with kernel methods, i.e. $\textsc{Rbf}$ kernel, would be a complicated, sometimes even impossible, task.)

\item As \textsc{Zilu} thinks higher of the classification correctness on the training data than anything else,
Before applying primitive $\textsc{Svm}$ technique, the parameters 
(mainly $C$ which tells the $\textsc{Svm}$ optimization how much you want to avoid misclassifying each training example)
%different from $\mathcal{C}$ used as loop invariant candidate in our context) 
should be tuned to get a classifier which does a better job of getting all the training points classified correctly.

\item What's more, we need to check the classification correctness of the learned classifier 
on $\mathcal{S}^+$ and $\mathcal{S}^-$ to ensure that is a perfect classifier.
Moreover, \textsc{Zilu} takes $\mathcal{S}^\rightarrow$ to validate the learned classifier(as is shown in Section~\ref{sec:sampling}).
\end{itemize} 

So the whole algorithm for classification can be described as Algorithm~\ref{alg:classify}.

\begin{algorithm}[!h]
\SetAlgoVlined
\Indm
\KwIn{$\mathcal{S}^+$, $\mathcal{S}^-$, and $\mathcal{S}^\rightarrow$}
\KwOut{$\textsc{Null}$ or a perfect classifier for $\mathcal{S}^+$ and $\mathcal{S}^-$ without violating $\mathcal{S}^\rightarrow$}
\Indp
    let $f$ = $\textsc{Svm}$($\mathcal{S}^+$, $\mathcal{S}^-)$\;
    \If {$f$ violates any data in $\mathcal{S}^+$ or $\mathcal{S}^-$} {
        \Return $\textsc{Null}$\;
    }
    \If {$f$ violates any inference rules in $\mathcal{S}^\rightarrow$} {
        \Return $\textsc{Null}$\;
    }
    \Return $f$;
\caption{Algorithm $classify$}
\label{alg:classify}
\end{algorithm}

Note that, in practice, 
customers can substitute $\textsc{Svm}$ in the algorithm~\ref{alg:classify} with other classification techniques for learning invariant. 
We will take two of them in ``$\textsc{Svm}$ derivatives'' part as examples.
 
%In the following, we present how we obtain a classifier automatically using $\textsc{Svm}$. 
%In this work, we always choose the \textit{optimal margin classifier} (see the definition in~\cite{Sharma2012}) if possible. 
%This half space could be seen as the strongest witness why the two data states are different. 
%In the following, we write $svm(S^+, S^-)$ to denote the function which returns a linear classifier

%\subsection{Checking}
%With the learned $\textsc{Svm}$ model,we check whether it can perfectly classify these states first.
%If yes, then we can automatically turn it back to a hyperplane form, which is regarded as our invariant candidate.
%Otherwise, we may apply other classification techniques for learning, which will be mentioned at ``$\textsc{Svm}$ derivatives'' part in the end of this section.


\subsection{Active Learning} 
After getting a classifier for the current dataset, active learning technique is used to refine the candidate until it gets convergence
(the candidate remains the same even when adding more data into then training set). 
Algorithm~\ref{alg:active} presents details on how active learning is implemented in \textsc{Zilu}. 

At line 3, we obtain a classifier based on Algorithm~\ref{alg:classify}. 
We compare the newly obtained classifier with the previous one at line 4, if they are identical, we return the classifier; 
otherwise, we apply selective sampling so that we can generate additional labeled samples for improving the classifier. 
In particular, at line 9, we apply standard techniques~\cite{DBLP:conf/icml/SchohnC00} to select the most informative sample. 
Notice that in our setting, as indicated in Section ~\ref{sec:sampling}, the most informative samples are those which are exactly on the lines 
and therefore can be obtained by solving an equation system using libraries. 
At line 10 and line 11, we test the program with the newly generated samples so as to label them accordingly.
\begin{algorithm}[t]
\SetAlgoVlined
\Indm
\KwIn{$\mathcal{S}^+$, $\mathcal{S}^-$, and $\mathcal{S}^\rightarrow$}
\KwOut{an invariant candidate $\mathcal{C}$}
\Indp
let $old_f$ be $null$\;
\While{true} {
    let $f$ = classify($\mathcal{S}^+$, $\mathcal{S}^-$, $\mathcal{S}^\rightarrow$)\;
    \If {$f$ is not equal to $\textsc{Null}$} {
        \If {$f$ is identical to $old_f$} {
            $\mathcal{C}$ = $f$\;
            \Return $\mathcal{C}$;
        }
        let $old_f = f$\;
    }
  %\textsc{Re-sampling}:\\
    $sam$ = selectiveSampling($old_f$)\;
    test the target program with $sam$\;
    update $\mathcal{S}^+$, $\mathcal{S}^-$, and $\mathcal{S}^\rightarrow$ accordingly\;
}
\caption{Algorithm $activeLearning$}
\label{alg:active}
\end{algorithm}


%\begin{example}
%\LL{to be added}
%\end{example}

\begin{proposition}
Algorithm $activeLearning$ always eventually terminates. \hfill \qed
\end{proposition}


\subsection{SVM Derivatives}
%$\mathcal{S}^+$, $\mathcal{S}^-$, and $\mathcal{S}^\rightarrow$
If $\mathcal{S}^+$, $\mathcal{S}^-$ cannot be perfectly classified by one half-space only, 
a more complicated function $f$ must be adopted. 
For instance, if there is a classifier in the form of conjunctive of multiple half spaces, 
the algorithm presented in~\cite{Sharma2012} can be used to identify such a classifier.

Also in algorithm~\ref{alg:classify}, we mentioned the customers can substitute $\textsc{Svm}$ with other classification approaches for invariant learning.
\textsc{Zilu} has implemented two algorithms besides native $\textsc{Svm}$: 
$Polynomial \textsc{Svm}$, which can be used to learn invariants in the form of polynomials or any equivalent expressions,
and $Conjunctive \textsc{Svm}$, which can learn invariants in the form of conjunctives.

\subsubsection{Polynomial SVM}
%In previous research, several papers ~\cite{**} have studied invariants with conjunctive form or disjunctive form.
%However, there is still no efficient approach to learning these invariants.
%In our research, we found sometimes convert the conjunctive or disjunctives to a polynomial expression might be a nice try to this problem.
In the real work, there are invariants of all kinds of form, one of which may be polynomial invariants, which have not been discussed by previous research.
In this part, we would like to learn this kind of invariants.
Apparently methods that use linear template or linear classification algorithm do not work.
%Actually, polynomials are more powerful on this than they look at the first glance, especially univariate polynomials. 
%Some cubic univariate polynomials can represent disjunctive of a conjunctive expression and a linear expression.
%For example, the following two expressions are equivalent:
%$$\big(x \ge x_0 \bigwedge x \le x_1) \bigvee x \ge x_2\big) \ where\ x_0 < x_1 < x_2$$
%$$x^3 + (x_0x_1 + x_0x_2 + x_1x_2)x^2 - (x_0 + x_1 + x_2)x - x_0x_1x_2 >= 0$$ 
This leads us to develop a classification algorithm for learning polynomial divider.
The primary idea is simple, after mapping raw data (program states) from original space in $\mathcal{S}^+$, $\mathcal{S}^-$ and $\mathcal{S}^\rightarrow$ to a high dimensions, 
we can apply primitive $\textsc{Svm}$ algorithm to learn a polynomial classifier. 
In practice, \textsc{Zilu} provide polynomials up to degree 4 as we think degree 4 can cover most hackneyed invariants for now.

\begin{algorithm}[!h]
\SetAlgoVlined
\Indm
\KwIn{$\mathcal{S}^+$, $\mathcal{S}^-$}
\KwOut{a perfect polynomial classifier for $\mathcal{S}^+$ and $\mathcal{S}^-$}
\Indp
    $max_dimension$ is pre-defined\;
    $dimension = 1$\;
    \While {$dimension \le max_dimention$} {
        $\mathcal{Q}^+$ = mapToDimension($\mathcal{S}^+$, dimension)\;
        $\mathcal{Q}^-$ = mapToDimension($\mathcal{S}^+$, dimension)\;
        let $f$ = $\textsc{Svm}$($\mathcal{Q}^+$, $\mathcal{Q}^-$)\;
        \If {$f$ does not violate any raw data in $\mathcal{S}^+$ or $\mathcal{S}^-$} {
        	\Return $f$\;
    	}
    	$dimension = dimension + 1$\;
    }
    \Return $\textsc{Null}$;
\caption{Algorithm $polynomialSVM$}
\label{alg:polynomialSVM}
\end{algorithm}

Along with polynomial invariants, we found there are also some invariant of conjunctive or disjunctive form which can be expressed in polynomials,
which means our approach can also find these sort of invariants.
For instance, if the target invariant is 
$$\big((x \ge x_0) \wedge (x \le x_1)\big) \vee (x \ge x_2) ~~~where\ x_0 < x_1 < x_2$$
which is really a challenge for current verification approaches to get,
we can find an equivalent polynomial expression:
$$x^3 + (x_0x_1 + x_0x_2 + x_1x_2)x^2 - (x_0 + x_1 + x_2)x - x_0x_1x_2 >= 0$$

%\begin{align}
%    x^3 + a\dot x^2 - b\dot x - c &>= &0 \\
%    where a &= & x_0 \dot x_1 + x_0 \dot x_2 + x_1 \dot x_2 \\
%\end{align}
%For instance, if the target invariant is 
%$$(x \ge x_0) \vee (x \le x_1) \ where\ x_0 < x_1$$
%we can find an equivalent polynomial expression:
%$$x^2 - (x_0 + x_1)x + x_0x_1 \ge 0$$ 
So $polynomialSVM$ can find a polynomial classifier for any complex expression as long as it can be expressed in a form of polynomials.


\subsubsection{Conjunctive SVM}
As stated in the last paragraph, polynomials can be equivalent with complex conjunctive or disjunctive of linear expressions.
%seems powerful in expression abilities, 
but there actually exists simple expressions that can not be expressed in form of polynomials.
For example, $\mathcal{C} = (x \ge 0 \wedge y \ge 0)$,
it can be proved that there is no such a polynomial which can be equivalent with it.

So in this case, we need to learn the conjunctive invariant directly, avoiding the tries to convert them into a form of polynomials.
Rahul Sharma, A. V. Nori et al. have developed a classification algorithm, named as `$\textsc{Svm\_i}$' in ~\cite{sharma2012interpolants},
based on $\textsc{Svm}$ to learn conjunctive invariants.
In their algorithm, they take $\textsc{Svm}$ as meta linear classification engine, 
and they fail to simplify the learned classifiers which can cause large number of conjunctive parts that can not be understood.
So in \textsc{Zilu}, we apply a technique shown in algorithm~\ref{alg:conjunctiveSVM} based on their $\textsc{Svm\_i}$ algorithm but overcome these defects:

\begin{algorithm}[!h]
\SetAlgoVlined
\Indm
\KwIn{$\mathcal{S}^+$, $\mathcal{S}^-$}
\KwOut{a set of perfect classifiers for $\mathcal{S}^+$ and $\mathcal{S}^-$}
\Indp
    let $\mathcal{C}$ = $\textsc{Null}$\;
    let $\textsc{Misclassified}$ = $\mathcal{S}^-$\;
    \While {$\textsc{Misclassified}$ is not empty} {
        Random choose $s$ from $\textsc{Misclassified}$\;
        let $f$ = $\textsc{Svm}$($\mathcal{S}^+$, s)\;
        add $f$ to $\mathcal{C}$\;
        \For {$s' \in \textsc{Misclassified}$} {\
            \If {$f(s') \le 0$} {
                remove $s'$ from $\textsc{Misclassified}$\;
            }
        }
    %}
    \For {$c \in \mathcal{C}$} {
        \If {$\mathcal{C}\diagdown c \Rightarrow c$} {
            remove $c$ from $\mathcal{C}$\;
        }
    }
    }
    \Return $\mathcal{C}$;
\caption{Algorithm $conjunctiveSVM$}
\label{alg:conjunctiveSVM}
\end{algorithm}



%\section{Active Learning}
%Due to the limited set of samples we have (which is often referred to as labeled samples in the machine learning community), 
%the guessed classifier obtained from the previous iteration might be far from being correct. 
%In fact, without labeled samples which are right on the boundary of the `actual' classifier, 
%it is very unlikely that we would find it. 
%Intuitively, in order to get the `actual' classifier, we would require samples which would distinguish the actual one from any nearby one. 
%This problem has been discussed and addressed in the machine learning community using active learning and selective sampling~\cite{DBLP:conf/icml/SchohnC00}.

%The concept of active learning or selective sampling refers to the approaches 
%that aim at reducing the labeling effort by selecting only the most informative samples to be labeled. 
%SVM selective sampling techniques have been proven effective in achieving a high accuracy 
%with fewer examples in many applications~\cite{DBLP:conf/mm/TongC01,DBLP:journals/jmlr/TongK01}. 
%The basic idea of selective sampling is that at each round, 
%we select the samples that are the closest to the classification boundary so that they are the most difficult to classify and the most informative to be labeled. 
%Since an SVM classification function is represented by support vectors which are the samples closest to the boundary, 
%this selective sampling effectively learns an accurate function with fewer labeled data~\cite{DBLP:conf/icml/SchohnC00}. 
%In our setting, this means that we should sample a program state right by the classifier and test the program 
%with that state to label that feature vector so that the classifier would be improved.


% section classification (end)