%!TEX root = paper.tex

\section{Introduction} % (fold)
\label{sec:introduction}

Automatic loop invariant generation is fundamental for program analysis. A loop invariant can be useful for software verification, compiler optimization, program understanding, etc. In the following, we first define the loop invariant generation problem and then briefly describe existing approaches and then our proposal. For simplicity, we assume that we are given a Hoare triple in the following form.
%\[
%    P = \{ \mathit{Pre} \} \mathit{while}(\mathit{Cond}) \{ \mathit{Body} \} \{ \mathit{Post} \}
%\]
\begin{align*}
&\{Pre\} & & /\star\text{\emph{Assumption}}\star/ \\
& while (Cond) \{ Body \} && /\star\text{\emph{Loop Body}}\star/\\
&\{ Post \} & & /\star\text{\emph{Assertion}}\star/
\end{align*}
Assume that $V = \{x_1{,} x_2{,} \cdots{,} x_n\}$ is a finite set of program variables which are relevant to the loop body. $Pre$, $Cond$ and $Post$ are predicates constituted by variables in $V$.

%\begin{align}
%&\{\mathit{Pre}\} & & \emph{Pre} \Rightarrow \emph{Inv} \label{org:inv:pre}\\
%&\mathit{while} (\mathit{Cond}) \{ \mathit{Body} \} && \{\emph{Inv} \wedge \emph{Cond}\} \emph{Body} \{\emph{Inv}\} \label{org:inv:loop}\\\
%&\{\mathit{Post}\} & & \emph{Inv} \wedge \neg \emph{Cond} \Rightarrow \emph{Post} \label{org:inv:post}
%\end{align}
% \begin{align}
% &\{\mathit{Pre}\} && \emph{Pre} \Rightarrow \emph{Inv} \label{sl1:org:inv:pre}\\
% &\mathit{while} (\mathit{Cond}) \{ \mathit{Body} \} && \{\emph{Inv} \wedge \emph{Cond}\} \emph{Body} \{\emph{Inv}\} \label{sl1:org:inv:loop}\\\
% &\{\mathit{Post}\} && \emph{Inv} \wedge \neg \emph{Cond} \Rightarrow \emph{Post} \label{sl1:org:inv:post}
% \end{align}
%In practice, the pre-condition $\mathit{Pre}$ is often described by
%the specification documents and checking conditions of the program inputs,
%and the post-condition $\mathit{Post}$ is usually specified
%by assertions and exceptions leading to an error state in the program.
Let $s = \{ x_1 \mapsto v_1, \cdots, x_n \mapsto v_n \}$ be a valuation of $V$. Let $\phi$ be a predicate constituted by variables in $V$. $\phi$ is viewed as the set of valuations of $V$ such that $\phi$ evaluates to true given the valuation. We thus write $s \in \phi$ to denote that $\phi$ is evaluated to $true$ given $s$. Otherwise, we write $s \not \in \phi$.
$Body$ is an imperative program which updates the valuation of $V$. For simplicity, we assume that it is a deterministic function\footnote{Our approach works as long as the non-determinism in $Body$ or $Cond$ is irrelevant to whether the postcondition is satisfied or not.} on valuations of variables $V$, and write $Body(s)$ to denote the valuation of $V$ after executing $Body$ given the initial variable valuation $s$. For convenience, $Body^i(s)$ where $i \geq 0$ is defined as follows: $Body^0(s) = s$ and $Body^{i+1}(s) = Body(Body^i(s))$.
%the evaluation function of the program variables $x_1, \ldots, x_n$
%and $\mathit{Body}(s)$ stand for their new evaluation after the execution of $\mathit{Body}$,
%the above program means that (1) $\mathit{Pre}$ is the assumption to the initial value of $s$;
%(2) if the $\mathit{Cond}$ is satisfied by $s$ at an iteration,
%$\mathit{Body}$ will be executed and $s$ will be updated to $\mathit{body}(s)$;
%(3) if the $\mathit{Cond}$ is unsatisfied by $s$ at an iteration,
%the while-loop ends and $s$ should satisfy $\mathit{Post}$.

The problem is thus either to prove the Hoare triple or to disprove it. In order to prove the Hoare triple, we would like to find a loop invariant $\mathit{Inv}$ which satisfies the following three conditions.
\begin{align}
    & Pre \subseteq Inv && \label{inv:pre} \\
    &\forall s.~s \in Inv \land Cond \implies Body(s) \in Inv && \label{inv:loop} \\
    & Inv \land \neg Cond \subseteq Post && \label{inv:post}
\end{align}
In order to disprove it, we would like to find a valuation $s$ such that $s \models \mathit{Pre}$ and executing the loop until it terminates results in a valuation $s'$ such that $s' \not \models \mathit{Post}$. For simplicity, we further assume that the loop body always terminates and refer the readers to~\cite{Domagoj:FAC:2013,LeQC:PLDI:15,Hong:ASE:2015} %%\cite{acmcomm}
for extensive research on proving loop termination.

Many approaches have been proposed to solve this problem.
For example, there are proposals based on abstraction interpretation~\cite{cousot1978automatic,mine2006octagon,karr1976affine,vincent2009subpolyhedra},
%% cousot1979systematic,
 counterexample guided abstraction refinement~\cite{henzinger2003software,thomas2001slam,edmund2003counterexample}, interpolation~\cite{kenneth2010lazy,thomas2004abstractions,kenneth2003interpolation,Kenneth2006lazy} and constraint solving and inference~\cite{ashutosh2009invgen,michael2003linear,sumit2009constraint}.
Recently, the authors of~\cite{sharma2012interpolants,sharma2013verification,DBLP:conf/esop/0001GHALN13,sharma2014invariant} proposed to automatically generate loop invariants based on random searching~\cite{sharma2014invariant} as well as machine learning~\cite{sharma2012interpolants}.
Their approaches start with randomly generating valuations of $V$ (a.k.a.~the samples) and categorize them into different groups, e.g., one containing those satisfying the loop invariant $\mathit{Inv}$ (if there is any) and another containing those not. Machine learning techniques are then used to generalize them in a certain form to obtain candidate loop invariants.
%For instance, classification algorithms like Support Vector Machines (SVM) ~\cite{sharma2012interpolants} can be used to generate classifiers as candidate invariants.
The candidates are then checked using program verification techniques (like symbolic execution~\cite{symbolic}) to see whether they satisfy the three conditions. If any of the conditions is violated, we obtain counterexamples in the form of variable valuations.
For instance, given a candidate $\phi$, if condition (1) is violated, a valuation $s \in (Pre \land \neg \phi)$ is generated, which proves that $\phi$ is not an invariant.
With the new sample $s$, we can re-classify the samples to obtain a new candidate invariant. This guess-and-check process is repeated until either the Hoare triple is proved or disproved.

One problem with the guess-and-check approach is that its effectiveness is often limited by the samples which are generated randomly.
In order to learn the right invariant through classification, often a large number of samples are necessary.
Furthermore, often those samples right by the boundary between variable valuations which satisfy the actual invariant and those which do not must be sampled so that classification techniques would identify the right invariant. Obtaining those samples through random sampling is often hard.
As a result, many iterations of guess-and-check are required. Another problem is that the kinds of loop invariants obtained through existing guess-and-check approaches~\cite{sharma2012interpolants,sharma2013verification,DBLP:conf/esop/0001GHALN13,sharma2014invariant} are often limited, e.g., to conjunctive linear inequalities~\cite{sharma2012interpolants} or equalities~\cite{DBLP:conf/esop/0001GHALN13}. Despite the approach presented in~\cite{DBLP:conf/pldi/GulwaniSV08,DBLP:conf/cav/SharmaDDA11}, learning disjunctive loop invariants remains a challenge.

In this work, we propose a technique to improve the existing guess-and-check approaches~\cite{sharma2012interpolants,sharma2013verification,DBLP:conf/esop/0001GHALN13,sharma2014invariant}.
 %We improve existing approaches in two aspects. First, by adopting active learning techniques, we improve the quality of the candidate invariants prior to verifying them, in every iteration of learn-and-check. As a result, we can reduce the number of learn-and-check iterations significantly. Second, by supporting an extensible framework, we can easily integrate different classification techniques (e.g., SVM with kernel methods~\cite{}) as well as the corresponding active learning techniques so that we can learn a large class of invariants. %We have developed a prototype implementation of our method and applied to benchmark programs including those from the software verification competition. The results show that our method often reduces the number of guess-and-check iterations as well as is able to learning more loop invariants than existing approaches.
%In the following, we define our problem and briefly illustrate how our approach works.
Compared to the existing approaches, we make the following contributions.
Firstly, we propose an active learning technique to overcome the limitation of random sampling.
That is, the active learning technique allows us to automatically generate samples which are important in improving the quality of the candidate invariants
so that we can improve the candidates prior to verifying them during every guess-and-check iteration.
As a result, we can reduce the number of guess-and-check iterations significantly, or even completely in many cases.
%    for automatic invariant inference based on machine learning.
%    Since the samples are chosen for clear purpose
%    to refine the invariant candidate in the \emph{data collection} stage,
%    the invariant converges efficiently.
%    Furthermore, because the counter-examples generated in the \emph{invariant verification} stage
%    give very accurate information to amend the invariant candidate,
%    they become a useful supplementary to overcome the weakness of machine learning
%    and fine-tune the invariant candidate.
Secondly, our approach is designed to be extensible so that we can learn different kinds of invariants.
For instance, we show that we can learn candidate invariants in the form of polynomial inequalities or their conjunctions using a different classification algorithm.
Furthermore, we show that by partitioning the samples according to the control locations they visit and classifying each partition separately, we are able to generate disjunctive invariants. Lastly, we implement our framework as a tool called \textsc{Zilu} (available at~\cite{zilu:repo}) and compare it with state-of-the-art tools like Interproc~\cite{jeannet2010interproc} as well as CPAChecker~\cite{DBLP:conf/cav/BeyerK11}.
%%    i.e.,
    %CPAChecker~\cite{beyer2011cpachecker} and
 %%   Interproc~\cite{jeannet2010interproc}.
%    Our experiment results show that
%    we are the only tool that can work with polynomial invariant inference.
%    Notice that the polynomial invariant inference works in our framework
%    naturally with very light additional programming.
    % Based on the design of different approaches,
    % we also claim that our framework have better extensibility comparing with their method.
%\textsc{Zilu} is built upon existing tools (e.g., GNU Scientific Library ($\mathit{GSL}$)~\cite{gough2009gnu} for active learning,   $\mathit{LibSVM}$~\cite{chang2011libsvm} for $\mathit{SVM}$ classification, revised $\mathit{KLEE}$~\cite{cadar2008klee} for symbolic execution~\cite{king1976symbolic,symbolic}, and Z3~\cite{de2008z3} for verification) and can be used as a language/platform independent tool to verify programs.

The remainders of the paper are organized as follows.
Section~\ref{sec:overview} presents an overview of our approach using an illustrative example.
Section~\ref{sec:classifierlearning} shows how candidate loop invariants are generated and refined through active learning.
%Section~\ref{sec:activelearning} then demonstrates the active learning technique which is applied to reduce the number of required samples.
Section~\ref{sec:evaluations} discusses our implementation and evaluates its effectiveness using a set of benchmark programs.
Section~\ref{sec:related} reviews related work and concludes.

