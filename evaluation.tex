\section{Evaluation} % (fold)
\label{sec:evaluations}
We have implemented our approach in a self-contained tool called \textsc{Zilu}, which is available at~\cite{zilu:repo}.
\textsc{Zilu} is written using a combination of C++ as well as shell codes (for invoking external tools). It makes use of GSL~\cite{gough2009gnu} to solve equation systems which is necessary for selective sampling and
LibSVM~\cite{chang2011libsvm} as a primitive classification engine for SVM-based classification. For verifying whether a candidate invariant can be used to prove the Hoare triple, we modify the KLEE project~\cite{cadar2008klee} to symbolically execute C programs prior to invoking Z3~\cite{de2008z3} for checking satisfiability of
condition (4), (5) and (6). We remark that KLEE is a symbolic executor; it may concretely execute the programs and return under-approximated abstraction. This may affect the soundness of our system.
To overcome this problem, we detect those path conditions produced from concrete executions and return sound abstraction (i.e., $true$). % for them.

Our evaluation subjects include all C programs we can find from previous publications (e.g.,~\cite{DBLP:conf/pldi/GulwaniSV08,sharma2012interpolants,gulavani2008automatically,jeannet2010interproc}) as well as software verification competitions~\cite{Dirk:SVCOMP:2016} (excluding those which cannot be made to satisfy our assumptions). All evaluated programs are available at~\cite{zilu:benchmark}. We remark that the loops in these benchmark programs often contain non-deterministic choices which are used to model I/O environment (e.g., an external function call). As non-determinism is beyond the scope of this work, we replace these non-deterministic commands with free boolean variables.

The parameters in our experiments are set as follows. For random sampling, we generate 10 random values of all input variables of a program from their default ranges. Furthermore, during selective sampling, we would add a few random samples in order to improve convergence. The default ratio between random samples and selective samples is 1:4. When we invoke LibSVM for classification, the parameter $C$ (which controls the trade-off between avoiding misclassifying training examples and enlarging decision boundary) and the inner iteration for SVM learning are set to their maximum value so that it generates only perfect classifiers.
During candidate verification, integer-type variables in programs are encoded as integers in Z3 (not as bit vectors). Since we have three ways of generating the candidate invariant, e.g., by setting the two sets of samples $P$ and $N$ differently as discussed in~Section~\ref{alternative}; or by using different classification algorithm (linear vs. polynomial or conjunctive vs. disjunctive), we simultaneously try to all combinations and terminate as soon as either the Hoare triple is proved or disproved. For polynomial classifiers, the maximum degree is set to be 4.

\begin{table}[t]
\scriptsize
\centering
\caption{Experiment results}
\begin{tabular}{l c | c c c | c c c | c }
\cline{3-8}
& &\multicolumn{3}{|c|}{\textsc{Zilu} + Selective Sampling}&\multicolumn{3}{c|}{\textsc{Zilu} - Selective Sampling} & \\
\hline
\multicolumn{1}{|c|}{benchmark}&\multicolumn{1}{|c|}{inv type}& $\sharp$sample & $\sharp$iteration & time(s) & $\sharp$sample & $\sharp$iteration &time(s) & \multicolumn{1}{|c|}{Interproc} \\
\hline % inserts single horizontal line
%\multicolumn{1}{|c|}{TEMPLATE} 		        	&polynomial 	& & &  &  &  &  & &  \\
%\hline
\multicolumn{1}{|c|}{f2~\cite{DBLP:conf/pldi/GulwaniSV08}}         						&linear 		&260 &1 &\textbf{10.15}  		&120 &1   &12.03  			&\multicolumn{1}{|c|}{\cmark} \\
\multicolumn{1}{|c|}{xy10~\cite{sharma2012interpolants}} 	        					&linear 		&810 &4 &\textbf{39.69} 		&840  &11  &40.51  			&\multicolumn{1}{|c|}{\cmark} \\
\multicolumn{1}{|c|}{xyz~\cite{sharma2012interpolants}}   	      					&linear 		&180 &1 &105.15  				&330  &5  &\textbf{93.04}  	&\multicolumn{1}{|c|}{\cmark} \\
\multicolumn{1}{|c|}{xy0\_1~\cite{sharma2012interpolants}}         					&conjunctive 	&1140 &20 &\textbf{51.07}		&1980 &48 &168.97  			&\multicolumn{1}{|c|}{\cmark} \\
\multicolumn{1}{|c|}{xy0\_2~\cite{sharma2012interpolants}}         					&conjunctive 	&180  &2 &16.09					&260 &6 &\textbf{15.98}  	&\multicolumn{1}{|c|}{\cmark} \\
\multicolumn{1}{|c|}{pldi08~\cite{gulavani2008automatically}} 		        			&disjunctive 	&- & - &timeout  				&-  &-  &timeout  		&\multicolumn{1}{|c|}{\xmark} \\

%\hline
\multicolumn{1}{|c|}{interproc1~\cite{jeannet2010interproc}}         				&linear 		&40 &1 &\textbf{9.21}  			&40 &2   &10.38  			&\multicolumn{1}{|c|}{\cmark} \\
\multicolumn{1}{|c|}{interproc2~\cite{jeannet2010interproc}}         				&linear 		&600 &1 &\textbf{11.66} 		&240  &1  &171.14  			&\multicolumn{1}{|c|}{\cmark} \\
\multicolumn{1}{|c|}{interproc3~\cite{jeannet2010interproc}}         				&linear 		&210 &1 &\textbf{30.32}  		&420 &5   &43.34  			&\multicolumn{1}{|c|}{\cmark} \\
\multicolumn{1}{|c|}{interproc4~\cite{jeannet2010interproc}}         				&linear 		&140 &4 &\textbf{4.8}  			&240 &5   &38.25  			&\multicolumn{1}{|c|}{\xmark} \\
\multicolumn{1}{|c|}{interproc5~\cite{jeannet2010interproc}}         				&linear 		&160 &1 &\textbf{9.18}  		&180 &3   &28.05  			&\multicolumn{1}{|c|}{\cmark} \\

%\hline
\multicolumn{1}{|c|}{zilu\_linear1}         			&linear 		&120 &2 &\textbf{24.79}  		&180 &2   &206.07  			&\multicolumn{1}{|c|}{\xmark} \\
\multicolumn{1}{|c|}{zilu\_linear2}         			&linear 		&420 &4 &\textbf{21.28}  		&720  &9  &82.24  			&\multicolumn{1}{|c|}{\cmark} \\
\multicolumn{1}{|c|}{zilu\_linear3}         			&linear 		&90 &1 &\textbf{16.19}  		&270 &3  &179.39  			&\multicolumn{1}{|c|}{\cmark} \\
\multicolumn{1}{|c|}{zilu\_linear4}         			&linear 		&140 &1 &\textbf{10.19}  		&420 &2  &24.51  			&\multicolumn{1}{|c|}{\cmark} \\
\multicolumn{1}{|c|}{zilu\_linear5}         			&linear 		&30 &1 &\textbf{8.57}  			&60 &3   &12.58  			&\multicolumn{1}{|c|}{\cmark} \\

\multicolumn{1}{|c|}{zilu\_poly1}         				&polynomial 	&50 &2 &\textbf{20.48}			&70 &1 &28.48  				&\multicolumn{1}{|c|}{\xmark} \\
\multicolumn{1}{|c|}{zilu\_poly2}         				&polynomial 	&110  &1 &\textbf{24.27}  		&70   &2 &41.62  			&\multicolumn{1}{|c|}{\xmark} \\
\multicolumn{1}{|c|}{zilu\_poly3}         				&polynomial 	&30 &1 &\textbf{11.83}  		&70  &2  &18.27  			&\multicolumn{1}{|c|}{\xmark} \\
\multicolumn{1}{|c|}{zilu\_poly4}         				&polynomial 	&- &-&timeout  				&-  &-  &timeout 			&\multicolumn{1}{|c|}{\xmark} \\
\multicolumn{1}{|c|}{zilu\_poly5}         				&polynomial 	&- &- &timeout  			&-  &-  &timeout  		&\multicolumn{1}{|c|}{\xmark} \\
\multicolumn{1}{|c|}{zilu\_poly6}         				&polynomial 	&300 &4 &\textbf{85.61}  		&620   &12 &472.68  		&\multicolumn{1}{|c|}{\xmark} \\

\multicolumn{1}{|c|}{zilu\_conj1}         				&conjunctive 	&120 &6 &\textbf{22.13}  		&220  &2  &97.47  			&\multicolumn{1}{|c|}{\xmark} \\
\multicolumn{1}{|c|}{zilu\_conj2}         				&conjunctive 	&280 &3 &\textbf{173.09}  		&-  &-  &timeout  		&\multicolumn{1}{|c|}{\cmark} \\

%\hline
\multicolumn{1}{|c|}{terminator\_01\_safe~\cite{beyer:SVCOMP:2013}}         		&linear 		&30 &1 &\textbf{9.2}  			&90  &4  &13.06  			&\multicolumn{1}{|c|}{\cmark} \\
%\hline
\multicolumn{1}{|c|}{afnp2014\_true~\cite{Dirk:SVCOMP:2016}}         			&conjunctive	&1240 &27 &\textbf{39.33}		&- &- &timeout  		&\multicolumn{1}{|c|}{\xmark} \\
\multicolumn{1}{|c|}{multivar\_true\_1~\cite{Dirk:SVCOMP:2016}}         		&conjunctive 	&340 &3 &16.84  				&220 &4   &\textbf{15.22}  	&\multicolumn{1}{|c|}{\cmark} \\
\multicolumn{1}{|c|}{cggmp2005\_variant~\cite{Dirk:SVCOMP:2016}}   				&conjunctive 	&1020 &13 &\textbf{108.57}		&- &- &timeout  		&\multicolumn{1}{|c|}{\cmark} \\
\multicolumn{1}{|c|}{css2003\_true~\cite{Dirk:SVCOMP:2016}}         			&conjunctive 	&1420 &33 &\textbf{57.78}		&5080 &125 &258.65  		&\multicolumn{1}{|c|}{\cmark} \\
\multicolumn{1}{|c|}{up\_true\_2~\cite{Dirk:SVCOMP:2016}}         				&conjunctive 	&1200 &14 &\textbf{84.02}  		&540 &7   &89.77  			&\multicolumn{1}{|c|}{\cmark} \\
\multicolumn{1}{|c|}{down\_true\_1~\cite{Dirk:SVCOMP:2016}}         			&conjunctive 	&1320  &15 &\textbf{116.21}  	&-  &-  &timeout  		&\multicolumn{1}{|c|}{\cmark} \\
\multicolumn{1}{|c|}{down\_true\_2~\cite{Dirk:SVCOMP:2016}}         			&conjunctive 	&1110 &14 &52.96  				&720 &10   &\textbf{44.99}  &\multicolumn{1}{|c|}{\cmark} \\

\hline
\end{tabular}
\label{tbl:stats}
\end{table}

The experiment results are presented in Table~\ref{tbl:stats}. All of the experiments are conducted using x64 Ubuntu 14.04.1 (kernel 3.19.0-59-generic) with 3.60 GHz Intel Core i7 and 32G DDR3.  In addition, we compare \textsc{Zilu} with a state-of-the-art invariant inference tool Interproc~\cite{jeannet2010interproc}. The first column in the table shows the name of the benchmark program as well as where it is from. The second shows column the type of invariant required for proving the Hoare triple. The next three columns present statistics of \textsc{Zilu} with selective sampling, i.e., the number of samples generated in total, the number of learn-and-check iterations and the total execution time. In order to show the relevance of active learning and selective sampling, we measure the performance of \textsc{Zilu}~\cite{zilu:repo} without selective sampling as well. The next three columns present the corresponding statistics of \textsc{Zilu} without selective sampling. 
The last column shows the result of an existing tool called Interproc, i.e., whether it generates a correct invariant. We do not show the time of Interproc because its result may not be correct and because it does not prove/disprove the Hoare triples. We remark that though many other approaches have been reported~\cite{sharma2012interpolants,sharma2013verification,DBLP:conf/esop/0001GHALN13,sharma2014invariant}, their tools are no longer not available for evaluation. Interproc generates invariants based on abstract interpretation. In the experiments, it is set to use its most expressive abstract domain, i.e., the reduced product of polyhedra and linear congruences abstraction. The comparison should be taken with a grain of salt as the methods are different. 

We have the following observations based on the experiment results. First, selective sampling is helpful in reducing the number of samples and learn-and-check iterations.
In the 25 experiments which \textsc{Zilu} work successfully without selective sampling, 19 of them need fewer or equal number of samples if apply selective sampling.
On average, the number of iterations is reduced from 10.84 to 4.96.
As a result, the time required for proving the program is often reduced.
\emph{We would like to highlight that for 12 out of 32 cases,
\textsc{Zilu} is able to learn the correct invariant with one iteration with selective sampling},
whereas only 3 programs can be verified with one iteration without selective sampling.
Since the invariant can be learnt in one iteration with a relatively high probability,
if the invariant inference process of a program timeouts because the symbolic verification is too complex,
the invariant returned by \textsc{Zilu} may still be the correct one.
This is particularly useful for handling real-world programs,
where we can manually verify the program using the candidate invariant generated by \textsc{Zilu}.


%whereas this is only the case for 3 programs without selective sampling.
%It implies that even if a program is too complicated to be verified through symbolic execution,
%\textsc{Zilu} may be able to learn the correct invariant with only random sampling and selective sampling.
%This is particularly useful for handling real-world programs, i.e., in such a case, \textsc{Zilu} may be able to generate an invariant which can be used to manually verify the program.
%\emph{We would like to highlight that for 12 out of 32 cases,
%\textsc{Zilu} is able to learn the correct invariant with one iteration with selective sampling},
%whereas only 3 programs can be verified with one iteration without selective sampling.
%Since the invariant can be learnt in one iteration with a relatively high probability,
%if the invariant inference process of a program timeouts because the symbolic verification is too complex,
%the invariant returned by \textsc{Zilu} may still be the correct one.
%This is particularly useful for handling real-world programs,
%where we can manually verify the program using the invariant generated by \textsc{Zilu}.


Second, \textsc{Zilu} is reasonably efficient. All loop invariants are learnt within three minutes.
It implies that selective sampling converges reasonably fast.
We observe that \textsc{Zilu} often takes more time to learn conjunctive invariants.
This is because Algorithm~\ref{alg:conjunctiveSVM} may invoke $\mathit{SVM}$ classification many times in one learn-and-check iteration.
\textsc{Zilu} times out in three cases. One is $pldi08$, which requires a disjunctive invariant which is not supported by \textsc{Zilu} currently.
The other two are \emph{zilu\_poly4} and \emph{zilu\_poly5}, which require polynomial invariants which are within the capability of \textsc{Zilu}.
Our investigation reveals that the reason is that after we match the program states to a higher dimension (using function $\mathit{mapToDegree}$),
the values needed for selective sampling in the higher-dimension space may not be feasible in the original space.
For example, assume a program with an integer variable $\mathit{x}$ which can be any value in set $\mathit{\{\cdots, -1, 0,  1, 2, 3, \cdots\}}$.
However $\mathit{(x, x^2)}$ is defined only in $\mathit{\{\cdots, (-1,1), (0,0), (1,1), (2, 4), (3, 9) \cdots\}}$ after mapping.
As a result, a sample selected according to selective sampling may often be infeasible, i.e., the equation we need to solve for selective sampling has no solution.

%What's more, the problem becomes much worse as the degree increases, as these undefined holes contains much more points than the defined samples, which is also the reason that we restrict the degree of our $polynomial$ algorithm up to 4.

%To illustrate the reason, consider the case of benchmark \emph{poly 1}, the loop invariant is $x^2 \le y^2$.
%However, the general form of an order-2 polynomial with 2 variables
%is: $a \cdot x^2 + b \cdot y^2 + c \cdot x y + d \cdot x + e \cdot y + f \geq 0$. It takes XXX iterations before $c$, $d$, $e$ and $f$ to converge to 0.
Lastly, the results show that \textsc{Zilu} complements existing tools. For instance,
\textsc{Zilu} can automatically generate polynomial, conjunctive or disjunctive loop invariants
which are often beyond the capability of Interproc.
Interproc is usually fast, i.e., it generates invariants within seconds. However, the invariant generated by Interproc may not be correct.
%This can be demonstrated using our running example introduced in Section~\ref{sec:introduction}.
%Since we have the loop condition $x < y$, it is impossible to execute the loop body when $(x \ge 0) \land (y < 0)$.
%However, Interproc outputs the loop invariant corresponding to this execution trace,
%because the invariants in Interproc are global constraints
%without considering their generation paths and conditions.

% section evaluations (end)
